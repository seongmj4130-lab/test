"""
Track A/B ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì ê²€ ìŠ¤í¬ë¦½íŠ¸

L0ë¶€í„° L7ê¹Œì§€ì˜ ê° ë‹¨ê³„ë³„ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì ê²€í•©ë‹ˆë‹¤.
"""

import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

def check_file_status(file_path, description=""):
    """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸"""
    if not file_path.exists():
        return {
            'exists': False,
            'size': 0,
            'modified': None,
            'description': description,
            'status': 'âŒ ëˆ„ë½'
        }

    stat = file_path.stat()
    size_mb = stat.st_size / 1024 / 1024

    # ìµœê·¼ ìˆ˜ì • ì‹œê°„ (24ì‹œê°„ ì´ë‚´)
    modified_time = datetime.fromtimestamp(stat.st_mtime)
    time_diff = datetime.now() - modified_time
    is_recent = time_diff.days < 1

    return {
        'exists': True,
        'size': size_mb,
        'modified': modified_time.strftime('%Y-%m-%d %H:%M:%S'),
        'is_recent': is_recent,
        'description': description,
        'status': 'âœ… ì¡´ì¬' + (' (ìµœê·¼)' if is_recent else '')
    }

def check_data_quality(file_path):
    """ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ ì ê²€"""
    if not file_path.exists():
        return None

    try:
        # íŒŒì¼ ì½ê¸°
        if file_path.suffix == '.parquet':
            df = pd.read_parquet(file_path)
        elif file_path.suffix == '.csv':
            df = pd.read_csv(file_path)
        else:
            return {'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹'}

        # ê¸°ë³¸ í†µê³„
        missing_rate = df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100

        return {
            'rows': len(df),
            'cols': len(df.columns),
            'missing_rate': missing_rate,
            'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'dtypes': df.dtypes.value_counts().to_dict()
        }

    except Exception as e:
        return {'error': str(e)}

def check_pipeline_stage(stage_num, stage_name, inputs, outputs, description=""):
    """ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì ê²€"""
    print(f"\n{'='*80}")
    print(f"ğŸ” L{stage_num}: {stage_name}")
    print(f"{'='*80}")
    print(f"ğŸ“ ì„¤ëª…: {description}")

    interim_dir = PROJECT_ROOT / 'data' / 'interim'

    # ì…ë ¥ ë°ì´í„° ì ê²€
    print(f"\nğŸ“¥ ì…ë ¥ ë°ì´í„°:")
    input_status = []
    for input_file in inputs:
        file_path = interim_dir / input_file
        status = check_file_status(file_path, f"L{stage_num} ì…ë ¥")
        input_status.append(status)
        print(f"  {input_file}: {status['status']}")

    # ì¶œë ¥ ë°ì´í„° ì ê²€
    print(f"\nğŸ“¤ ì¶œë ¥ ë°ì´í„°:")
    output_status = []
    for output_file in outputs:
        file_path = interim_dir / output_file
        status = check_file_status(file_path, f"L{stage_num} ì¶œë ¥")
        output_status.append(status)
        print(f"  {output_file}: {status['status']}")

        # ë°ì´í„° í’ˆì§ˆ ì ê²€
        if status['exists']:
            quality = check_data_quality(file_path)
            if quality and 'error' not in quality:
                print(f"    ğŸ“Š í¬ê¸°: {quality['rows']:,}í–‰ x {quality['cols']}ì—´")
                print(".1f")
                print(".1f")
            elif quality and 'error' in quality:
                print(f"    âŒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {quality['error']}")

    # ë‹¨ê³„ë³„ ì‹¤í–‰ ê°€ëŠ¥ì„± í‰ê°€
    input_ready = all(s['exists'] for s in input_status)
    output_ready = all(s['exists'] for s in output_status)

    if input_ready and not output_ready:
        exec_status = "ğŸŸ¡ ì‹¤í–‰ í•„ìš”"
    elif input_ready and output_ready:
        exec_status = "âœ… ì‹¤í–‰ ì™„ë£Œ"
    elif not input_ready:
        exec_status = "âŒ ì…ë ¥ ë°ì´í„° ëˆ„ë½"
    else:
        exec_status = "â“ ìƒíƒœ ë¶ˆëª…"

    print(f"\nğŸ¯ ì‹¤í–‰ ìƒíƒœ: {exec_status}")

    return {
        'stage': stage_num,
        'name': stage_name,
        'inputs': input_status,
        'outputs': output_status,
        'execution_status': exec_status,
        'input_ready': input_ready,
        'output_ready': output_ready
    }

def check_ensemble_config():
    """ì•™ìƒë¸” ì„¤ì • ìƒíƒœ ì ê²€"""
    print(f"\n{'='*80}")
    print("ğŸ”§ ì•™ìƒë¸” ì„¤ì • ìƒíƒœ ì ê²€")
    print(f"{'='*80}")

    try:
        from src.utils.config import load_config
        cfg = load_config('configs/config.yaml')

        l5 = cfg.get('l5', {})
        model_type = l5.get('model_type', 'single')
        print(f"ëª¨ë¸ íƒ€ì…: {model_type}")

        if model_type == 'ensemble':
            print("âœ… ì•™ìƒë¸” ëª¨ë“œ í™œì„±í™”")

            short_weights = l5.get('ensemble_weights_short', {})
            long_weights = l5.get('ensemble_weights_long', {})

            print("\në‹¨ê¸° í˜¸ë¦¬ì¦Œ ê°€ì¤‘ì¹˜:")
            if short_weights:
                for model, weight in short_weights.items():
                    print(".3f")
                total_short = sum(short_weights.values())
                print(".3f"            else:
                print("  âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

            print("\nì¥ê¸° í˜¸ë¦¬ì¦Œ ê°€ì¤‘ì¹˜:")
            if long_weights:
                for model, weight in long_weights.items():
                    print(".3f")
                total_long = sum(long_weights.values())
                print(".3f"            else:
                print("  âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

            # ê°€ì¤‘ì¹˜ ê²€ì¦
            short_valid = abs(total_short - 1.0) < 0.01 if short_weights else False
            long_valid = abs(total_long - 1.0) < 0.01 if long_weights else False

            if short_valid and long_valid:
                print("âœ… ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì¦ í†µê³¼")
            else:
                print("âš ï¸ ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì¦ ì‹¤íŒ¨ (í•©ê³„ê°€ 1.0ì´ ì•„ë‹˜)")
        else:
            print("âš ï¸ ë‹¨ì¼ ëª¨ë¸ ëª¨ë“œ (ì•™ìƒë¸” ë¹„í™œì„±í™”)")

    except Exception as e:
        print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

def generate_pipeline_report(results):
    """ì¢…í•© íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œ ìƒì„±"""
    print(f"\n{'='*100}")
    print("ğŸ“‹ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…í•© ë³´ê³ ì„œ")
    print(f"{'='*100}")

    # ë‹¨ê³„ë³„ ìš”ì•½
    summary_data = []
    for result in results:
        summary_data.append({
            'ë‹¨ê³„': f"L{result['stage']}",
            'ì´ë¦„': result['name'],
            'ì…ë ¥ì¤€ë¹„': 'âœ…' if result['input_ready'] else 'âŒ',
            'ì¶œë ¥ì¤€ë¹„': 'âœ…' if result['output_ready'] else 'âŒ',
            'ì‹¤í–‰ìƒíƒœ': result['execution_status']
        })

    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))

    # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë¶„ì„
    total_stages = len(results)
    completed_stages = sum(1 for r in results if r['execution_status'] == 'âœ… ì‹¤í–‰ ì™„ë£Œ')
    ready_stages = sum(1 for r in results if r['execution_status'] == 'ğŸŸ¡ ì‹¤í–‰ í•„ìš”')
    blocked_stages = sum(1 for r in results if r['execution_status'] == 'âŒ ì…ë ¥ ë°ì´í„° ëˆ„ë½')

    print("
ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë¶„ì„:"    print(f"  ì´ ë‹¨ê³„: {total_stages}")
    print(f"  ì™„ë£Œ: {completed_stages}")
    print(f"  ì‹¤í–‰ ê°€ëŠ¥: {ready_stages}")
    print(f"  ì°¨ë‹¨ë¨: {blocked_stages}")

    # íŒŒì´í”„ë¼ì¸ ê±´ê°•ë„
    health_score = (completed_stages / total_stages) * 100
    print(".1f"

    if health_score >= 80:
        health_status = "âœ… ê±´ê°•í•¨"
    elif health_score >= 60:
        health_status = "ğŸŸ¡ ë³´í†µ"
    elif health_score >= 40:
        health_status = "âš ï¸ ì£¼ì˜ í•„ìš”"
    else:
        health_status = "âŒ ì‹¬ê°í•œ ë¬¸ì œ"

    print(f"  ê±´ê°•ë„: {health_status}")

    # ì‹¤í–‰ ê°€ëŠ¥ ë‹¨ê³„ ì‹ë³„
    executable_stages = [r for r in results if r['execution_status'] == 'ğŸŸ¡ ì‹¤í–‰ í•„ìš”']
    if executable_stages:
        print("
ğŸŸ¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„:"        for stage in executable_stages:
            print(f"  - L{stage['stage']}: {stage['name']}")

    # ì°¨ë‹¨ëœ ë‹¨ê³„ ì‹ë³„
    blocked_stages_list = [r for r in results if r['execution_status'] == 'âŒ ì…ë ¥ ë°ì´í„° ëˆ„ë½']
    if blocked_stages_list:
        print("
âŒ ì°¨ë‹¨ëœ ë‹¨ê³„:"        for stage in blocked_stages_list:
            print(f"  - L{stage['stage']}: {stage['name']}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¬ Track A/B ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì ê²€")
    print("="*100)
    print(f"ì ê²€ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ê° ë‹¨ê³„ë³„ ì ê²€
    pipeline_results = []

    # L0: Universe êµ¬ì„±
    result_l0 = check_pipeline_stage(
        0, "Universe êµ¬ì„±",
        [],  # ì™¸ë¶€ ë°ì´í„° ì‚¬ìš©
        ["universe_k200_membership_monthly.parquet"],
        "KOSPI200 ì¢…ëª© ì„ ì • ë° ë©¤ë²„ì‹­ ë°ì´í„° ìƒì„±"
    )
    pipeline_results.append(result_l0)

    # L1: OHLCV ë° ê¸°ìˆ ì§€í‘œ
    result_l1 = check_pipeline_stage(
        1, "OHLCV ë° ê¸°ìˆ ì§€í‘œ",
        ["universe_k200_membership_monthly.parquet"],
        ["dataset_daily.parquet"],
        "ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ë° 20ê°œ+ ê¸°ìˆ ì§€í‘œ ê³„ì‚°"
    )
    pipeline_results.append(result_l1)

    # L2: ì¬ë¬´ ë°ì´í„°
    result_l2 = check_pipeline_stage(
        2, "ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘",
        ["dataset_daily.parquet"],
        [],  # ì¬ë¬´ ë°ì´í„°ëŠ” dataset_dailyì— ë³‘í•©ë¨
        "DART APIë¥¼ í†µí•œ ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘"
    )
    pipeline_results.append(result_l2)

    # L3: íŒ¨ë„ ë°ì´í„° ë³‘í•©
    result_l3 = check_pipeline_stage(
        3, "íŒ¨ë„ ë°ì´í„° ë³‘í•©",
        ["dataset_daily.parquet"],  # ì¬ë¬´ ë°ì´í„°ê°€ í¬í•¨ëœ ìƒíƒœ
        ["dataset_daily.parquet"],  # ë™ì¼ íŒŒì¼ ì—…ë°ì´íŠ¸
        "OHLCV, ê¸°ìˆ ì§€í‘œ, ì¬ë¬´ ë°ì´í„°ë¥¼ í†µí•©"
    )
    pipeline_results.append(result_l3)

    # L4: CV í´ë“œ ë¶„í• 
    result_l4 = check_pipeline_stage(
        4, "Walk-Forward CV ë¶„í• ",
        ["dataset_daily.parquet"],
        ["cv_folds_short.parquet", "cv_folds_long.parquet", "targets_and_folds.parquet"],
        "ì‹œê³„ì—´ CV í´ë“œ ìƒì„± (ë‹¨ê¸° 20ì¼, ì¥ê¸° 120ì¼)"
    )
    pipeline_results.append(result_l4)

    # L5: ML ëª¨ë¸ í•™ìŠµ
    result_l5 = check_pipeline_stage(
        5, "ML ëª¨ë¸ í•™ìŠµ",
        ["dataset_daily.parquet", "cv_folds_short.parquet", "cv_folds_long.parquet", "targets_and_folds.parquet"],
        ["pred_short_oos.parquet", "pred_long_oos.parquet"],
        "Grid, Ridge, XGBoost, RF ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"
    )
    pipeline_results.append(result_l5)

    # L6: ìŠ¤ì½”ì–´ ìƒì„±
    result_l6 = check_pipeline_stage(
        6, "ìŠ¤ì½”ì–´ ìƒì„± ë° ì•™ìƒë¸”",
        ["pred_short_oos.parquet", "pred_long_oos.parquet"],
        ["rebalance_scores.parquet"],
        "ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ì„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” ìŠ¤ì½”ì–´ë¡œ ë³€í™˜"
    )
    pipeline_results.append(result_l6)

    # L7: ë°±í…ŒìŠ¤íŠ¸
    result_l7 = check_pipeline_stage(
        7, "ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
        ["rebalance_scores.parquet", "targets_and_folds.parquet", "dataset_daily.parquet"],
        ["bt_metrics_*.parquet", "bt_positions_*.parquet", "bt_equity_curve.parquet"],
        "4ê°œ ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ë° ì„±ê³¼ ë¶„ì„"
    )
    pipeline_results.append(result_l7)

    # ì•™ìƒë¸” ì„¤ì • ì ê²€
    check_ensemble_config()

    # ì¢…í•© ë³´ê³ ì„œ
    generate_pipeline_report(pipeline_results)

    print(f"\nğŸ† ì ê²€ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
