# -*- coding: utf-8 -*-
"""
ì¬í˜„ì„± ê²€ì¦ì„ ìœ„í•œ 3ë²ˆ ì™„ì „ ì¬ì‹¤í–‰

L5-L7 íŒŒì´í”„ë¼ì¸ì„ 3ë²ˆ ì™„ì „íˆ ì¬ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ì¼ê´€ì„± ê²€ì¦
"""

from pathlib import Path
import pandas as pd
import numpy as np
import shutil
import time
from datetime import datetime
import sys
import subprocess

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

def backup_existing_results():
    """ê¸°ì¡´ ê²°ê³¼ ë°±ì—…"""
    print("ğŸ“‹ ê¸°ì¡´ ê²°ê³¼ ë°±ì—… ì¤‘...")

    interim_dir = PROJECT_ROOT / 'data' / 'interim'
    backup_dir = PROJECT_ROOT / 'data' / 'backup_before_reproducibility_test'
    backup_dir.mkdir(parents=True, exist_ok=True)

    # ë°±ì—…í•  íŒŒì¼ë“¤
    files_to_backup = [
        'pred_short_oos.parquet',
        'pred_long_oos.parquet',
        'rebalance_scores.parquet',
        'rebalance_scores_original.parquet',
        'bt_metrics_bt20_ens.parquet',
        'bt_metrics_bt20_short.parquet',
        'bt_metrics_bt120_ens.parquet',
        'bt_metrics_bt120_long.parquet'
    ]

    for file in files_to_backup:
        src = interim_dir / file
        if src.exists():
            dst = backup_dir / f"{file}.backup"
            shutil.copy2(src, dst)
            print(f"  âœ… {file} ë°±ì—… ì™„ë£Œ")

    print("ğŸ“‹ ë°±ì—… ì™„ë£Œ\n")

def clear_ml_cache():
    """ML ìºì‹œ ë° ì¤‘ê°„ ê²°ê³¼ ì‚­ì œ"""
    print("ğŸ—‘ï¸ ML ìºì‹œ ë° ì¤‘ê°„ ê²°ê³¼ ì‚­ì œ ì¤‘...")

    interim_dir = PROJECT_ROOT / 'data' / 'interim'

    # ì‚­ì œí•  íŒŒì¼ë“¤ (L5-L7 ê´€ë ¨)
    files_to_delete = [
        'pred_short_oos.parquet',
        'pred_long_oos.parquet',
        'rebalance_scores.parquet',
        'bt_metrics_bt20_ens.parquet',
        'bt_metrics_bt20_short.parquet',
        'bt_metrics_bt120_ens.parquet',
        'bt_metrics_bt120_long.parquet'
    ]

    for file in files_to_delete:
        file_path = interim_dir / file
        if file_path.exists():
            file_path.unlink()
            print(f"  âœ… {file} ì‚­ì œ ì™„ë£Œ")

    print("ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\n")

def run_single_pipeline_run(run_id, seed=None):
    """ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"ğŸš€ ì¬ì‹¤í–‰ #{run_id} ì‹œì‘")
    print("="*60)

    start_time = time.time()

    try:
        # L5-L7 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        cmd = [sys.executable, 'scripts/run_l5_l7_pipeline.py']
        if seed:
            cmd.extend(['--seed', str(seed)])

        result = subprocess.run(cmd, capture_output=True, text=True, cwd=PROJECT_ROOT)

        if result.returncode != 0:
            print(f"âŒ ì‹¤í–‰ #{run_id} ì‹¤íŒ¨")
            print(f"stdout: {result.stdout}")
            print(f"stderr: {result.stderr}")
            return None

        # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ
        interim_dir = PROJECT_ROOT / 'data' / 'interim'
        results = {}

        bt_files = [
            'bt_metrics_bt20_ens.parquet',
            'bt_metrics_bt20_short.parquet',
            'bt_metrics_bt120_ens.parquet',
            'bt_metrics_bt120_long.parquet'
        ]

        for bt_file in bt_files:
            file_path = interim_dir / bt_file
            if file_path.exists():
                df = pd.read_parquet(file_path)
                results[bt_file.replace('.parquet', '')] = df
            else:
                print(f"âš ï¸ {bt_file} ìƒì„± ì‹¤íŒ¨")
                return None

        execution_time = time.time() - start_time
        print(".2f"
        return results, execution_time

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ #{run_id} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return None

def save_run_results(run_results, run_id):
    """ì‹¤í–‰ ê²°ê³¼ ì €ì¥"""
    results_dir = PROJECT_ROOT / 'artifacts' / 'reports' / 'reproducibility_test_results'
    results_dir.mkdir(parents=True, exist_ok=True)

    for strategy, df in run_results.items():
        filename = f"{strategy}_run_{run_id}.parquet"
        filepath = results_dir / filename
        df.to_parquet(filepath, index=False)
        print(f"  ğŸ’¾ {filename} ì €ì¥ ì™„ë£Œ")

def run_reproducibility_test():
    """ì¬í˜„ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ”¬ ì¬í˜„ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. ê¸°ì¡´ ê²°ê³¼ ë°±ì—…
    backup_existing_results()

    # 2. ìºì‹œ ì •ë¦¬
    clear_ml_cache()

    # 3. 3ë²ˆ ì¬ì‹¤í–‰
    all_results = {}
    execution_times = {}

    for run_id in range(1, 4):
        print(f"\n{'='*80}")
        print(f"ğŸ”„ RUN {run_id}/3")
        print('='*80)

        # ì„œë¡œ ë‹¤ë¥¸ ì‹œë“œë¡œ ì¬í˜„ì„± í…ŒìŠ¤íŠ¸
        seed = 42 + run_id  # 43, 44, 45

        result = run_single_pipeline_run(run_id, seed)
        if result:
            run_results, exec_time = result
            all_results[f'run_{run_id}'] = run_results
            execution_times[f'run_{run_id}'] = exec_time

            # ê²°ê³¼ ì €ì¥
            save_run_results(run_results, run_id)

            # ë‹¤ìŒ ì‹¤í–‰ ì „ ìºì‹œ ì •ë¦¬ (ì™„ì „í•œ ì¬í˜„ì„± ë³´ì¥)
            if run_id < 3:
                clear_ml_cache()
        else:
            print(f"âŒ RUN {run_id} ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return None

    return all_results, execution_times

def analyze_reproducibility_results(all_results, execution_times):
    """ì¬í˜„ì„± ê²°ê³¼ ë¶„ì„"""
    print("
ğŸ“Š ì¬í˜„ì„± ë¶„ì„ ê²°ê³¼"    print("="*80)

    if not all_results:
        print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê° ì „ëµë³„ë¡œ ì‹¤í–‰ ê°„ ì°¨ì´ ë¶„ì„
    strategies = ['bt_metrics_bt20_ens', 'bt_metrics_bt20_short', 'bt_metrics_bt120_ens', 'bt_metrics_bt120_long']

    reproducibility_metrics = {}

    for strategy in strategies:
        print(f"\nğŸ¯ {strategy} ì¬í˜„ì„± ë¶„ì„")
        print("-" * 50)

        run_values = []
        for run_id in range(1, 4):
            run_key = f'run_{run_id}'
            if run_key in all_results and strategy in all_results[run_key]:
                df = all_results[run_key][strategy]
                if len(df) > 0:
                    # Holdout ê²°ê³¼ë§Œ ì‚¬ìš© (ë” ì•ˆì •ì )
                    holdout_data = df[df['phase'] == 'holdout']
                    if len(holdout_data) > 0:
                        metrics = {
                            'sharpe': holdout_data['net_sharpe'].iloc[0],
                            'cagr': holdout_data['net_cagr'].iloc[0],
                            'mdd': holdout_data['net_mdd'].iloc[0],
                            'calmar': holdout_data['net_calmar_ratio'].iloc[0]
                        }
                        run_values.append(metrics)
                        print(f"  RUN {run_id}: Sharpe={metrics['sharpe']:.4f}, CAGR={metrics['cagr']:.4f}, MDD={metrics['mdd']:.4f}")

        if len(run_values) == 3:
            # ê° ì§€í‘œë³„ ë³€ë™ì„± ê³„ì‚°
            sharpe_values = [v['sharpe'] for v in run_values]
            cagr_values = [v['cagr'] for v in run_values]
            mdd_values = [v['mdd'] for v in run_values]
            calmar_values = [v['calmar'] for v in run_values]

            reproducibility_metrics[strategy] = {
                'sharpe_std': np.std(sharpe_values),
                'sharpe_cv': np.std(sharpe_values) / np.mean(sharpe_values) if np.mean(sharpe_values) != 0 else 0,
                'cagr_std': np.std(cagr_values),
                'cagr_cv': np.std(cagr_values) / np.mean(cagr_values) if np.mean(cagr_values) != 0 else 0,
                'mdd_std': np.std(mdd_values),
                'mdd_cv': np.std(mdd_values) / abs(np.mean(mdd_values)) if np.mean(mdd_values) != 0 else 0,
                'calmar_std': np.std(calmar_values),
                'calmar_cv': np.std(calmar_values) / np.mean(calmar_values) if np.mean(calmar_values) != 0 else 0
            }

            print(".6f"            print(".4f"            print(".6f"            print(".4f"            print(".6f"            print(".4f"            print(".6f"            print(".4f"        else:
            print(f"  âŒ {strategy}: 3ë²ˆ ì‹¤í–‰ ê²°ê³¼ ì¤‘ {len(run_values)}ê°œë§Œ ì„±ê³µ")

    # ì¢…í•© ì¬í˜„ì„± í‰ê°€
    print("
ğŸ† ì¢…í•© ì¬í˜„ì„± í‰ê°€"    print("="*80)

    if reproducibility_metrics:
        # í‰ê·  ë³€ë™ê³„ìˆ˜ë¡œ ì¬í˜„ì„± í‰ê°€
        avg_cv_sharpe = np.mean([m['sharpe_cv'] for m in reproducibility_metrics.values()])
        avg_cv_cagr = np.mean([m['cagr_cv'] for m in reproducibility_metrics.values()])
        avg_cv_mdd = np.mean([m['mdd_cv'] for m in reproducibility_metrics.values()])
        avg_cv_calmar = np.mean([m['calmar_cv'] for m in reproducibility_metrics.values()])

        print(".4f"        print(".4f"        print(".4f"        print(".4f"
        # ì¬í˜„ì„± ë“±ê¸‰ í‰ê°€
        def get_reproducibility_grade(cv):
            if cv < 0.01: return "â­â­â­â­â­ ì™„ë²½"
            elif cv < 0.05: return "â­â­â­â­ ìš°ìˆ˜"
            elif cv < 0.10: return "â­â­â­ ì–‘í˜¸"
            elif cv < 0.20: return "â­â­ ë³´í†µ"
            else: return "â­ ê°œì„  í•„ìš”"

        print("
ğŸ“‹ ì¬í˜„ì„± ë“±ê¸‰:"        print(f"  Sharpe: {get_reproducibility_grade(avg_cv_sharpe)}")
        print(f"  CAGR: {get_reproducibility_grade(avg_cv_cagr)}")
        print(f"  MDD: {get_reproducibility_grade(avg_cv_mdd)}")
        print(f"  Calmar: {get_reproducibility_grade(avg_cv_calmar)}")

        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        print("
â±ï¸ ì‹¤í–‰ ì‹œê°„ ë¶„ì„:"        for run_id in range(1, 4):
            run_key = f'run_{run_id}'
            if run_key in execution_times:
                print(".2f"
        avg_time = np.mean(list(execution_times.values()))
        time_std = np.std(list(execution_times.values()))
        print(".2f"        print(".2f"
        # ìµœì¢… ê²°ë¡ 
        overall_cv = np.mean([avg_cv_sharpe, avg_cv_cagr, avg_cv_mdd, avg_cv_calmar])
        if overall_cv < 0.05:
            conclusion = "âœ… ì¬í˜„ì„± ìš°ìˆ˜: 3ë²ˆ ì¬ì‹¤í–‰ ê²°ê³¼ê°€ ë§¤ìš° ì¼ê´€ë¨"
        elif overall_cv < 0.10:
            conclusion = "ğŸŸ¡ ì¬í˜„ì„± ì–‘í˜¸: ì•½ê°„ì˜ ë³€ë™ì„± ìˆì§€ë§Œ ì•ˆì •ì "
        elif overall_cv < 0.20:
            conclusion = "ğŸŸ  ì¬í˜„ì„± ë³´í†µ: ì¶”ê°€ ê²€í†  í•„ìš”"
        else:
            conclusion = "âŒ ì¬í˜„ì„± ì €ì¡°: ì‹œìŠ¤í…œ ê°œì„  í•„ìš”"

        print(f"\nğŸ¯ ìµœì¢… ê²°ë¡ : {conclusion}")
        print(".4f"
    else:
        print("âŒ ì¬í˜„ì„± ë¶„ì„ ì‹¤íŒ¨: ê²°ê³¼ ë°ì´í„° ë¶€ì¡±")

def restore_backup():
    """ë°±ì—… íŒŒì¼ ë³µì›"""
    print("
ğŸ”„ ë°±ì—… íŒŒì¼ ë³µì› ì¤‘..."    backup_dir = PROJECT_ROOT / 'data' / 'backup_before_reproducibility_test'
    interim_dir = PROJECT_ROOT / 'data' / 'interim'

    if backup_dir.exists():
        backup_files = list(backup_dir.glob('*.backup'))
        for backup_file in backup_files:
            original_name = backup_file.name.replace('.backup', '')
            dst = interim_dir / original_name
            shutil.copy2(backup_file, dst)
            print(f"  âœ… {original_name} ë³µì› ì™„ë£Œ")

        print("ğŸ”„ ë°±ì—… ë³µì› ì™„ë£Œ")
    else:
        print("âš ï¸ ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ì¬í˜„ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = run_reproducibility_test()

        if test_results:
            all_results, execution_times = test_results

            # ê²°ê³¼ ë¶„ì„
            analyze_reproducibility_results(all_results, execution_times)

        # ë°±ì—… ë³µì›
        restore_backup()

        print(f"\nğŸ† ì¬í˜„ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()

        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°±ì—… ë³µì›
        restore_backup()

if __name__ == "__main__":
    main()