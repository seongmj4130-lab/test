# -*- coding: utf-8 -*-
"""
Track A ì‚°ì¶œë¬¼ì„ CSVë¡œ ì €ì¥
- ë‚ ì§œ ë²”ìœ„: 2023-01-01 ~ 2024-12-31
- ì»¬ëŸ¼: ë‚ ì§œ, ì¢…ëª©ëª…(í‹°ì»¤), ìŠ¤ì½”ì–´, top3 ì˜í–¥ íŒ©í„°ì…‹(ì ˆëŒ“ê°’)
"""
from __future__ import annotations

import pandas as pd
import numpy as np
from pathlib import Path
import yaml
from typing import Dict, List, Tuple
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.components.ranking.score_engine import (
    normalize_feature_cross_sectional,
    _pick_feature_cols,
)
from src.utils.feature_groups import get_feature_groups, load_feature_groups


# íŒ©í„° ê·¸ë£¹ í•œê¸€ëª… ë§¤í•‘
FACTOR_GROUP_NAMES = {
    "technical": "ê¸°ìˆ ì ë¶„ì„",
    "value": "ê°€ì¹˜",
    "profitability": "ìˆ˜ìµì„±",
    "news": "ë‰´ìŠ¤",
    "other": "ê¸°íƒ€",
    "esg": "ESG",
}


def load_feature_weights(weights_config_path: Path) -> Dict[str, float]:
    """í”¼ì²˜ ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ"""
    if not weights_config_path.exists():
        print(f"ê²½ê³ : ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weights_config_path}")
        return {}
    
    with open(weights_config_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f) or {}
    
    return data.get("feature_weights", {})


def calculate_feature_contributions(
    df: pd.DataFrame,
    feature_cols: List[str],
    feature_weights: Dict[str, float],
    normalization_method: str = "percentile",
    sector_col: str = None,
    use_sector_relative: bool = True,
) -> pd.DataFrame:
    """
    ê° ì¢…ëª©/ë‚ ì§œë³„ë¡œ íŒ©í„° ê¸°ì—¬ë„ ê³„ì‚°
    
    Returns:
        ì›ë³¸ dfì— ê° íŒ©í„°ì˜ ê¸°ì—¬ë„ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    out = df.copy()
    
    # sector-relative ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    actual_sector_col = None
    if use_sector_relative and sector_col and sector_col in out.columns:
        if out[sector_col].notna().sum() > 0:
            actual_sector_col = sector_col
    
    # ê° íŒ©í„°ì˜ ì •ê·œí™”ëœ ê°’ ê³„ì‚°
    normalized_features = {}
    for feat in feature_cols:
        if feat not in out.columns:
            continue
        
        normalized = normalize_feature_cross_sectional(
            out,
            feat,
            "date",
            method=normalization_method,
            sector_col=actual_sector_col,
        )
        normalized_features[feat] = normalized
    
    # ê° íŒ©í„°ì˜ ê¸°ì—¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ê°’ Ã— ê°€ì¤‘ì¹˜)
    contribution_cols = {}
    for feat in normalized_features.keys():
        weight = feature_weights.get(feat, 0.0)
        contribution = normalized_features[feat] * weight
        contribution_cols[feat] = contribution
        out[f"contrib_{feat}"] = contribution
    
    return out


def get_feature_to_group_mapping(
    feature_groups_config: Path,
) -> Dict[str, str]:
    """
    í”¼ì²˜ëª…ì„ ê·¸ë£¹ëª…ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    
    Returns:
        {í”¼ì²˜ëª…: ê·¸ë£¹ëª…} ë”•ì…”ë„ˆë¦¬
    """
    if not feature_groups_config.exists():
        return {}
    
    cfg_groups = load_feature_groups(feature_groups_config)
    feature_groups = get_feature_groups(cfg_groups)
    
    mapping = {}
    for group_name, features in feature_groups.items():
        for feat in features:
            mapping[str(feat)] = group_name
    
    return mapping


def get_top3_factor_groups(
    row: pd.Series,
    feature_cols: List[str],
    feature_to_group: Dict[str, str],
    prefix: str = "contrib_",
) -> Tuple[str, str, str]:
    """
    í•œ í–‰ì—ì„œ ì ˆëŒ“ê°’ ê¸°ì¤€ top3 íŒ©í„° ê·¸ë£¹ ì¶”ì¶œ (í•œê¸€ëª…)
    
    Returns:
        (top1, top2, top3) íŠœí”Œ (íŒ©í„° ê·¸ë£¹ í•œê¸€ëª…)
    """
    contributions = {}
    for feat in feature_cols:
        col = f"{prefix}{feat}"
        if col in row.index:
            val = row[col]
            if pd.notna(val):
                # ê·¸ë£¹ëª…ìœ¼ë¡œ ë³€í™˜
                group_name = feature_to_group.get(feat, "other")
                # ê·¸ë£¹ë³„ ê¸°ì—¬ë„ í•©ì‚° (ê°™ì€ ê·¸ë£¹ì— ì†í•œ ì—¬ëŸ¬ í”¼ì²˜ì˜ ê¸°ì—¬ë„ í•©ì‚°)
                if group_name not in contributions:
                    contributions[group_name] = 0.0
                contributions[group_name] += abs(val)
    
    if len(contributions) == 0:
        return ("", "", "")
    
    # ì ˆëŒ“ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_groups = sorted(contributions.items(), key=lambda x: x[1], reverse=True)
    
    # í•œê¸€ëª…ìœ¼ë¡œ ë³€í™˜
    top3 = []
    for group_name, _ in sorted_groups[:3]:
        korean_name = FACTOR_GROUP_NAMES.get(group_name, group_name)
        top3.append(korean_name)
    
    # ë¶€ì¡±í•œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
    while len(top3) < 3:
        top3.append("")
    
    return tuple(top3[:3])


def get_stock_names(tickers: List[str]) -> Dict[str, str]:
    """
    í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ì¢…ëª©ëª… ë”•ì…”ë„ˆë¦¬ ìƒì„±
    
    Returns:
        {í‹°ì»¤: ì¢…ëª©ëª…} ë”•ì…”ë„ˆë¦¬
    """
    try:
        import pykrx.stock as stock
    except ImportError:
        print("ê²½ê³ : pykrxê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì¢…ëª©ëª… ì—†ì´ í‹°ì»¤ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
        return {ticker: "" for ticker in tickers}
    
    stock_names = {}
    unique_tickers = sorted(set(tickers))
    
    print(f"  - ì¢…ëª©ëª… ì¡°íšŒ ì¤‘ ({len(unique_tickers)}ê°œ í‹°ì»¤)...")
    for i, ticker in enumerate(unique_tickers):
        try:
            name = stock.get_market_ticker_name(ticker)
            stock_names[ticker] = name if name else ""
        except Exception as e:
            stock_names[ticker] = ""
            if (i + 1) % 50 == 0:
                print(f"    ì§„í–‰: {i+1}/{len(unique_tickers)}")
    
    print(f"  - ì¢…ëª©ëª… ì¡°íšŒ ì™„ë£Œ: {sum(1 for v in stock_names.values() if v)}ê°œ ì„±ê³µ")
    return stock_names


def export_track_a_to_csv(
    ranking_file: str = "data/interim/ranking_short_daily.parquet",
    dataset_file: str = "data/interim/dataset_daily.parquet",
    weights_config: str = "configs/feature_weights_short_hitratio_optimized.yaml",
    groups_config: str = "configs/feature_groups_short.yaml",
    output_file: str = "data/processed/track_a_output_2023_2024.csv",
    start_date: str = "2023-01-01",
    end_date: str = "2024-12-31",
    normalization_method: str = "percentile",
):
    """
    Track A ì‚°ì¶œë¬¼ì„ CSVë¡œ ì €ì¥
    
    Args:
        ranking_file: ranking_short_daily.parquet ê²½ë¡œ
        dataset_file: dataset_daily.parquet ê²½ë¡œ (í”¼ì²˜ ë°ì´í„°)
        weights_config: í”¼ì²˜ ê°€ì¤‘ì¹˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        output_file: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
        start_date: ì‹œì‘ ë‚ ì§œ
        end_date: ì¢…ë£Œ ë‚ ì§œ
        normalization_method: ì •ê·œí™” ë°©ë²•
    """
    project_root = Path(__file__).resolve().parent.parent
    ranking_path = project_root / ranking_file
    dataset_path = project_root / dataset_file
    weights_path = project_root / weights_config
    groups_path = project_root / groups_config
    output_path = project_root / output_file
    
    print(f"[1/5] ë°ì´í„° ë¡œë“œ ì¤‘...")
    print(f"  - Ranking: {ranking_path}")
    print(f"  - Dataset: {dataset_path}")
    
    # ë­í‚¹ ë°ì´í„° ë¡œë“œ
    ranking_df = pd.read_parquet(ranking_path)
    ranking_df["date"] = pd.to_datetime(ranking_df["date"])
    
    # ë‚ ì§œ í•„í„°ë§
    start_dt = pd.to_datetime(start_date)
    end_dt = pd.to_datetime(end_date)
    ranking_df = ranking_df[
        (ranking_df["date"] >= start_dt) & (ranking_df["date"] <= end_dt)
    ].copy()
    
    print(f"  - ë­í‚¹ ë°ì´í„°: {len(ranking_df):,}í–‰, {ranking_df['date'].nunique()}ê°œ ë‚ ì§œ")
    
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ (í”¼ì²˜ í¬í•¨)
    dataset_df = pd.read_parquet(dataset_path)
    dataset_df["date"] = pd.to_datetime(dataset_df["date"])
    
    # ë‚ ì§œ í•„í„°ë§
    dataset_df = dataset_df[
        (dataset_df["date"] >= start_dt) & (dataset_df["date"] <= end_dt)
    ].copy()
    
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(dataset_df):,}í–‰")
    
    # ë­í‚¹ê³¼ ì›ë³¸ ë°ì´í„° ë³‘í•©
    print(f"[2/5] ë°ì´í„° ë³‘í•© ì¤‘...")
    merged_df = ranking_df.merge(
        dataset_df,
        on=["date", "ticker"],
        how="inner",
    )
    print(f"  - ë³‘í•© ê²°ê³¼: {len(merged_df):,}í–‰")
    
    # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
    print(f"[3/5] í”¼ì²˜ ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ê¸°ì—¬ë„ ê³„ì‚° ì¤‘...")
    feature_cols = _pick_feature_cols(merged_df)
    print(f"  - ì‚¬ìš© í”¼ì²˜: {len(feature_cols)}ê°œ")
    
    # í”¼ì²˜ ê°€ì¤‘ì¹˜ ë¡œë“œ
    feature_weights = load_feature_weights(weights_path)
    print(f"  - ê°€ì¤‘ì¹˜ ë¡œë“œ: {len(feature_weights)}ê°œ")
    
    # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” í”¼ì²˜ëŠ” ì œì™¸
    feature_cols_with_weights = [f for f in feature_cols if f in feature_weights and feature_weights[f] != 0]
    print(f"  - ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” í”¼ì²˜: {len(feature_cols_with_weights)}ê°œ")
    
    if len(feature_cols_with_weights) == 0:
        raise ValueError("ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # sector_col í™•ì¸
    sector_col = None
    if "sector_name" in merged_df.columns:
        if merged_df["sector_name"].notna().sum() > 0:
            sector_col = "sector_name"
    
    # íŒ©í„° ê¸°ì—¬ë„ ê³„ì‚°
    merged_with_contrib = calculate_feature_contributions(
        merged_df,
        feature_cols_with_weights,
        feature_weights,
        normalization_method=normalization_method,
        sector_col=sector_col,
        use_sector_relative=True,
    )
    
    # í”¼ì²˜-ê·¸ë£¹ ë§¤í•‘ ë¡œë“œ
    print(f"[4/6] í”¼ì²˜ ê·¸ë£¹ ë§¤í•‘ ë¡œë“œ ì¤‘...")
    feature_to_group = get_feature_to_group_mapping(groups_path)
    print(f"  - ê·¸ë£¹ ë§¤í•‘: {len(feature_to_group)}ê°œ í”¼ì²˜")
    
    print(f"[5/6] Top3 íŒ©í„° ê·¸ë£¹ ì¶”ì¶œ ì¤‘...")
    # Top3 íŒ©í„° ê·¸ë£¹ ì¶”ì¶œ (í•œê¸€ëª…)
    top3_groups = merged_with_contrib.apply(
        lambda row: get_top3_factor_groups(row, feature_cols_with_weights, feature_to_group),
        axis=1,
    )
    
    # ì¢…ëª©ëª… ì¡°íšŒ
    print(f"[6/6] ì¢…ëª©ëª… ì¡°íšŒ ì¤‘...")
    unique_tickers = merged_with_contrib["ticker"].unique().tolist()
    stock_names = get_stock_names(unique_tickers)
    
    # ì¢…ëª©ëª…ê³¼ í‹°ì»¤ ê²°í•©
    merged_with_contrib["stock_name"] = merged_with_contrib["ticker"].map(stock_names)
    merged_with_contrib["ì¢…ëª©ëª…_í‹°ì»¤"] = merged_with_contrib.apply(
        lambda row: f"{row['stock_name']}({row['ticker']})" if row['stock_name'] else row['ticker'],
        axis=1,
    )
    
    # ê²°ê³¼ DataFrame êµ¬ì„±
    result_df = pd.DataFrame({
        "ë‚ ì§œ": merged_with_contrib["date"].dt.strftime("%Y-%m-%d"),
        "ì¢…ëª©ëª…(í‹°ì»¤)": merged_with_contrib["ì¢…ëª©ëª…_í‹°ì»¤"],
        "ìŠ¤ì½”ì–´": merged_with_contrib["score_total"],
        "Top1_íŒ©í„°ê·¸ë£¹": [f[0] for f in top3_groups],
        "Top2_íŒ©í„°ê·¸ë£¹": [f[1] for f in top3_groups],
        "Top3_íŒ©í„°ê·¸ë£¹": [f[2] for f in top3_groups],
    })
    
    # Top3 íŒ©í„°ì…‹ì„ í•˜ë‚˜ì˜ ì»¬ëŸ¼ìœ¼ë¡œ í•©ì¹˜ê¸° (ì˜ˆ: "íŒ©í„°1|íŒ©í„°2|íŒ©í„°3")
    result_df["Top3_ì˜í–¥_íŒ©í„°ì…‹"] = result_df.apply(
        lambda row: "|".join([f for f in [row["Top1_íŒ©í„°ê·¸ë£¹"], row["Top2_íŒ©í„°ê·¸ë£¹"], row["Top3_íŒ©í„°ê·¸ë£¹"]] if f]),
        axis=1,
    )
    
    # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
    final_df = result_df[["ë‚ ì§œ", "ì¢…ëª©ëª…(í‹°ì»¤)", "ìŠ¤ì½”ì–´", "Top3_ì˜í–¥_íŒ©í„°ì…‹"]].copy()
    
    # ì •ë ¬ (ë‚ ì§œ, ìŠ¤ì½”ì–´ ë‚´ë¦¼ì°¨ìˆœ)
    final_df = final_df.sort_values(["ë‚ ì§œ", "ìŠ¤ì½”ì–´"], ascending=[True, False])
    
    print(f"[7/7] CSV ì €ì¥ ì¤‘...")
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # CSV ì €ì¥
    final_df.to_csv(output_path, index=False, encoding="utf-8-sig")
    
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"  - ì¶œë ¥ íŒŒì¼: {output_path}")
    print(f"  - ì´ í–‰ ìˆ˜: {len(final_df):,}")
    print(f"  - ë‚ ì§œ ë²”ìœ„: {final_df['ë‚ ì§œ'].min()} ~ {final_df['ë‚ ì§œ'].max()}")
    print(f"  - ì¢…ëª© ìˆ˜: {final_df['ì¢…ëª©ëª…(í‹°ì»¤)'].nunique()}ê°œ")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 10í–‰):")
    print(final_df.head(10).to_string(index=False))


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Track A ì‚°ì¶œë¬¼ì„ CSVë¡œ ì €ì¥")
    parser.add_argument("--ranking", type=str, default="data/interim/ranking_short_daily.parquet",
                       help="ë­í‚¹ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--dataset", type=str, default="data/interim/dataset_daily.parquet",
                       help="ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--weights", type=str, default="configs/feature_weights_short_hitratio_optimized.yaml",
                       help="í”¼ì²˜ ê°€ì¤‘ì¹˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--groups", type=str, default="configs/feature_groups_short.yaml",
                       help="í”¼ì²˜ ê·¸ë£¹ ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="data/processed/track_a_output_2023_2024.csv",
                       help="ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--start-date", type=str, default="2023-01-01",
                       help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, default="2024-12-31",
                       help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    
    args = parser.parse_args()
    
    export_track_a_to_csv(
        ranking_file=args.ranking,
        dataset_file=args.dataset,
        weights_config=args.weights,
        groups_config=args.groups,
        output_file=args.output,
        start_date=args.start_date,
        end_date=args.end_date,
    )

