# -*- coding: utf-8 -*-
"""
L0~L4 ê³µí†µë°ì´í„° í’ˆì§ˆ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ê³„ë³„ ì‚°ì¶œë¬¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ë¶„ì„í•˜ê³  ë¬¸ì œì ì„ íŒŒì•…í•©ë‹ˆë‹¤.
"""

from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import sys

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

def analyze_data_file(file_path, stage_name, expected_cols=None):
    """ë‹¨ì¼ ë°ì´í„° íŒŒì¼ ë¶„ì„"""
    print(f"\nğŸ“Š {stage_name} ë¶„ì„")
    print("-" * 50)

    # ë””ë²„ê¹…: íŒŒì¼ ê²½ë¡œ ì¶œë ¥
    print(f"íŒŒì¼ ê²½ë¡œ: {file_path}")
    print(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {file_path.exists()}")

    if not file_path.exists():
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        return None

    try:
        # íŒŒì¼ ì½ê¸° (parquet ìš°ì„ , ì—†ìœ¼ë©´ csv)
        parquet_path = Path(str(file_path) + '.parquet')
        csv_path = Path(str(file_path) + '.csv')

        if parquet_path.exists():
            df = pd.read_parquet(parquet_path)
        elif csv_path.exists():
            df = pd.read_csv(csv_path)
        else:
            print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
            return None

        print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df):,}í–‰ x {len(df.columns)}ì—´")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")

        # ê¸°ë³¸ ì •ë³´
        print(f"   ë‚ ì§œ ë²”ìœ„: {df.index.min() if isinstance(df.index, pd.DatetimeIndex) else 'N/A'} ~ {df.index.max() if isinstance(df.index, pd.DatetimeIndex) else 'N/A'}")

        # ì»¬ëŸ¼ ì •ë³´
        if expected_cols:
            missing_cols = set(expected_cols) - set(df.columns)
            if missing_cols:
                print(f"   âš ï¸ ëˆ„ë½ëœ ì˜ˆìƒ ì»¬ëŸ¼: {missing_cols}")

        # ê²°ì¸¡ì¹˜ ë¶„ì„
        missing_analysis = analyze_missing_values(df, stage_name)

        # ë°ì´í„° íƒ€ì… ë¶„ì„
        dtype_analysis = analyze_data_types(df)

        return {
            'dataframe': df,
            'row_count': len(df),
            'col_count': len(df.columns),
            'missing_analysis': missing_analysis,
            'dtype_analysis': dtype_analysis
        }

    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_missing_values(df, stage_name):
    """ê²°ì¸¡ì¹˜ ë¶„ì„"""
    print(f"\n   ğŸ” ê²°ì¸¡ì¹˜ ë¶„ì„:")

    # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜
    missing_by_col = df.isnull().sum()
    missing_cols = missing_by_col[missing_by_col > 0]

    if len(missing_cols) == 0:
        print("   âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        return {'status': 'clean', 'missing_rate': 0.0}

    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë“¤
    total_cells = len(df) * len(df.columns)
    total_missing = missing_by_col.sum()
    missing_rate = total_missing / total_cells

    print(".1%")
    print(f"   ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜: {len(missing_cols)}/{len(df.columns)}")

    # ìƒìœ„ ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ë“¤
    top_missing = missing_cols.nlargest(10)
    print("   ì£¼ìš” ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ (Top 10):")
    for col, count in top_missing.items():
        rate = count / len(df) * 100
        print(".1f")

    # ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
    missing_pattern = analyze_missing_patterns(df)

    return {
        'status': 'has_missing' if missing_rate > 0 else 'clean',
        'missing_rate': missing_rate,
        'missing_cols': len(missing_cols),
        'total_missing': total_missing,
        'top_missing_cols': top_missing.to_dict(),
        'pattern_analysis': missing_pattern
    }

def analyze_missing_patterns(df):
    """ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„"""
    missing_matrix = df.isnull()

    # í–‰ë³„ ê²°ì¸¡ì¹˜
    missing_by_row = missing_matrix.sum(axis=1)
    rows_with_missing = (missing_by_row > 0).sum()
    rows_missing_rate = rows_with_missing / len(df) * 100

    # ì™„ì „ ê²°ì¸¡ í–‰
    complete_missing_rows = (missing_by_row == len(df.columns)).sum()

    # ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ (ì£¼ìš” ì»¬ëŸ¼ë“¤ë§Œ)
    if len(df.columns) > 50:
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìƒ˜í”Œë§
        sample_cols = df.columns[:20]  # ìƒìœ„ 20ê°œ ì»¬ëŸ¼ë§Œ
        corr_matrix = missing_matrix[sample_cols].corr()
    else:
        corr_matrix = missing_matrix.corr()

    # ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìŒ ì°¾ê¸°
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.8:  # 0.8 ì´ìƒ ìƒê´€ê´€ê³„
                high_corr_pairs.append((
                    corr_matrix.columns[i],
                    corr_matrix.columns[j],
                    corr_val
                ))

    return {
        'rows_with_missing': rows_with_missing,
        'rows_missing_rate': rows_missing_rate,
        'complete_missing_rows': complete_missing_rows,
        'high_corr_missing_pairs': high_corr_pairs[:10]  # ìƒìœ„ 10ê°œë§Œ
    }

def analyze_data_types(df):
    """ë°ì´í„° íƒ€ì… ë¶„ì„"""
    dtype_counts = df.dtypes.value_counts()

    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¶„ì„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        numeric_stats = df[numeric_cols].describe()

        # ì´ìƒì¹˜ ë¶„ì„ (IQR ë°©ë²•)
        outlier_analysis = {}
        for col in numeric_cols[:10]:  # ìƒìœ„ 10ê°œë§Œ
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            outlier_rate = outliers / len(df) * 100

            if outlier_rate > 1:  # 1% ì´ìƒ ì´ìƒì¹˜
                outlier_analysis[col] = {
                    'outlier_count': outliers,
                    'outlier_rate': outlier_rate,
                    'bounds': [lower_bound, upper_bound]
                }

    return {
        'dtype_counts': dtype_counts.to_dict(),
        'numeric_cols': len(numeric_cols),
        'outlier_analysis': outlier_analysis if 'outlier_analysis' in locals() else {}
    }

def analyze_l0_l4_pipeline():
    """L0~L4 íŒŒì´í”„ë¼ì¸ ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
    print("ğŸ”¬ L0~L4 ê³µí†µë°ì´í„° í’ˆì§ˆ ë¶„ì„")
    print("="*80)
    print(f"ë¶„ì„ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    interim_dir = PROJECT_ROOT / 'data' / 'interim'

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ë§Œ ë¶„ì„
    interim_dir = PROJECT_ROOT / 'data' / 'interim'

    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
    existing_files = []
    potential_files = [
        ('L5_predictions_short', 'pred_short_oos', ['date', 'ticker', 'pred']),
        ('L5_predictions_long', 'pred_long_oos', ['date', 'ticker', 'pred']),
        ('L6_rebalance_scores', 'rebalance_scores', ['date', 'ticker', 'score_ens'])
    ]

    for stage_name, file_base, expected_cols in potential_files:
        parquet_file = interim_dir / f"{file_base}.parquet"
        csv_file = interim_dir / f"{file_base}.csv"

        if parquet_file.exists() or csv_file.exists():
            existing_files.append({
                'stage': stage_name,
                'file': file_base,
                'expected_cols': expected_cols
            })

    analysis_targets = existing_files

    results = {}

    for target in analysis_targets:
        file_path = interim_dir / target['file']
        result = analyze_data_file(
            file_path,
            target['stage'],
            target.get('expected_cols')
        )
        if result:
            results[target['stage']] = result

    # ì¢…í•© ë¶„ì„
    generate_summary_report(results)

    return results

def generate_summary_report(results):
    """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    print("\nğŸ“‹ ì¢…í•© í’ˆì§ˆ ë¶„ì„ ë³´ê³ ì„œ")
    print("="*80)

    if not results:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‹¨ê³„ë³„ ìš”ì•½
    summary_data = []
    for stage, result in results.items():
        missing_rate = result['missing_analysis']['missing_rate'] * 100
        missing_cols = result['missing_analysis']['missing_cols']

        status = "âœ… ì–‘í˜¸"
        if missing_rate > 10:
            status = "âŒ ì‹¬ê°"
        elif missing_rate > 5:
            status = "âš ï¸ ì£¼ì˜"
        elif missing_rate > 1:
            status = "ğŸ”¶ ë³´í†µ"

        summary_data.append({
            'ë‹¨ê³„': stage,
            'í–‰ìˆ˜': result['row_count'],
            'ì—´ìˆ˜': result['col_count'],
            'ê²°ì¸¡ë¥ (%)': ".1f",
            'ê²°ì¸¡ì»¬ëŸ¼ìˆ˜': missing_cols,
            'ìƒíƒœ': status
        })

    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))

    # ë¬¸ì œì  ë¶„ì„
    print("\nğŸ¯ ì£¼ìš” ë¬¸ì œì  ë¶„ì„")
    print("-"*50)

    total_stages = len(results)
    clean_stages = sum(1 for r in results.values() if r['missing_analysis']['status'] == 'clean')
    problematic_stages = total_stages - clean_stages

    print(f"ì´ ë¶„ì„ ë‹¨ê³„: {total_stages}")
    print(f"í´ë¦° ë‹¨ê³„: {clean_stages}")
    print(f"ë¬¸ì œ ë‹¨ê³„: {problematic_stages}")

    # ê²°ì¸¡ì¹˜ ì‹¬ê°ë„ ë¶„ì„
    severe_missing = sum(1 for r in results.values() if r['missing_analysis']['missing_rate'] > 0.1)
    moderate_missing = sum(1 for r in results.values() if 0.05 < r['missing_analysis']['missing_rate'] <= 0.1)

    print("\nê²°ì¸¡ì¹˜ ì‹¬ê°ë„:")
    print(f"  ì‹¬ê°(>10%): {severe_missing}ë‹¨ê³„")
    print(f"  ë³´í†µ(5-10%): {moderate_missing}ë‹¨ê³„")
    print(f"  ê²½ë¯¸(<5%): {problematic_stages - severe_missing - moderate_missing}ë‹¨ê³„")

    # ë°ì´í„° í™œìš©ì„± í‰ê°€
    usability_score = (clean_stages / total_stages) * 100
    print(".1f")
    if usability_score >= 80:
        print("âœ… ë°ì´í„° í™œìš©ì„±: ë†’ìŒ")
    elif usability_score >= 60:
        print("âš ï¸ ë°ì´í„° í™œìš©ì„±: ë³´í†µ")
    else:
        print("âŒ ë°ì´í„° í™œìš©ì„±: ë‚®ìŒ")

    # íŒŒì¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = PROJECT_ROOT / 'artifacts' / 'reports' / f'l0_l4_data_quality_analysis_{timestamp}.csv'
    summary_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    analyze_l0_l4_pipeline()

    print("\nğŸ† ë¶„ì„ ì™„ë£Œ")
    print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()