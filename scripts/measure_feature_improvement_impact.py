# -*- coding: utf-8 -*-
"""
í”¼ì³ ê°œì„  íš¨ê³¼ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸

ê°œì„  ì „í›„ IC ì„±ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict

import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

from src.features.feature_unit_tester import FeatureUnitTester
from src.utils.config import load_config
from src.utils.io import load_artifact, save_artifact


def measure_baseline_performance():
    """
    ê°œì„  ì „ ê¸°ì¤€ ì„±ê³¼ ì¸¡ì •

    Returns:
        ê¸°ì¤€ ì„±ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("ğŸ“Š ê¸°ì¤€ ì„±ê³¼ ì¸¡ì • ì‹œì‘...")

    # ì„¤ì • ë¡œë“œ
    cfg = load_config('configs/config.yaml')
    interim_dir = Path(cfg['paths']['base_dir']) / 'data' / 'interim'

    # CV folds ë¡œë“œ
    cv_folds = load_artifact(interim_dir / 'cv_folds_short')

    # ê¸°ë³¸ í”¼ì³ë“¤ë¡œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
    baseline_features = [
        'close', 'volume', 'price_momentum', 'price_momentum_60d',
        'momentum_3m', 'momentum_6m', 'volatility_60d', 'volatility_20d',
        'net_income', 'roe', 'debt_ratio', 'turnover'
    ]

    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì¤€ë¹„
    panel_df = load_artifact(interim_dir / 'panel_merged_daily')
    rebalance_df = load_artifact(interim_dir / 'rebalance_scores_from_ranking')

    if panel_df is None or rebalance_df is None:
        return None

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì³ë“¤ë§Œ
    available_features = [f for f in baseline_features if f in panel_df.columns]

    feature_data = panel_df[['date', 'ticker'] + available_features].copy()
    target_data = rebalance_df[['date', 'ticker', 'true_short']].copy()
    target_data = target_data.rename(columns={'true_short': 'ret_fwd_20d'})

    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = FeatureUnitTester()
    baseline_results = tester.test_feature_set(
        feature_data, target_data, cv_folds, available_features, 'short'
    )

    if len(baseline_results) > 0:
        # ì¢…í•© IC ê³„ì‚°
        avg_ic = baseline_results['holdout_ic_mean'].mean()
        avg_hit = baseline_results['holdout_hit_ratio'].mean()

        baseline_performance = {
            'features_tested': len(baseline_results),
            'avg_ic': avg_ic,
            'avg_hit_ratio': avg_hit,
            'top_feature_ic': baseline_results['holdout_ic_mean'].max(),
            'feature_details': baseline_results.to_dict('records')
        }

        print(".4f")
        print(".1%")
        return baseline_performance

    return None


def measure_improved_performance():
    """
    ê°œì„  í›„ ì„±ê³¼ ì¸¡ì •

    Returns:
        ê°œì„  ì„±ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("ğŸš€ ê°œì„  ì„±ê³¼ ì¸¡ì • ì‹œì‘...")

    # ì„¤ì • ë¡œë“œ
    cfg = load_config('configs/config.yaml')
    interim_dir = Path(cfg['paths']['base_dir']) / 'data' / 'interim'

    # CV folds ë¡œë“œ
    cv_folds = load_artifact(interim_dir / 'cv_folds_short')

    # ê°œì„ ëœ í”¼ì³ë“¤
    improved_features = [
        # ê¸°ì¡´ í”¼ì³ë“¤
        'close', 'volume', 'price_momentum', 'price_momentum_60d',
        'momentum_3m', 'momentum_6m', 'volatility_60d', 'volatility_20d',
        'net_income', 'roe', 'debt_ratio', 'turnover',
        # ìƒˆë¡œ ì¶”ê°€ëœ í”¼ì³ë“¤
        'close_to_52w_high', 'close_to_52w_low', 'intraday_price_position',
        'momentum_3m_ewm', 'momentum_6m_ewm', 'momentum_3m_vol_adj',
        'volatility_asymmetry', 'tail_risk_5pct',
        'news_intensity', 'news_trend'
    ]

    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì¤€ë¹„
    panel_df = load_artifact(interim_dir / 'panel_merged_daily')
    rebalance_df = load_artifact(interim_dir / 'rebalance_scores_from_ranking')

    if panel_df is None or rebalance_df is None:
        return None

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì³ë“¤ë§Œ
    available_features = [f for f in improved_features if f in panel_df.columns]

    feature_data = panel_df[['date', 'ticker'] + available_features].copy()
    target_data = rebalance_df[['date', 'ticker', 'true_short']].copy()
    target_data = target_data.rename(columns={'true_short': 'ret_fwd_20d'})

    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = FeatureUnitTester()
    improved_results = tester.test_feature_set(
        feature_data, target_data, cv_folds, available_features, 'short'
    )

    if len(improved_results) > 0:
        # ì¢…í•© IC ê³„ì‚°
        avg_ic = improved_results['holdout_ic_mean'].mean()
        avg_hit = improved_results['holdout_hit_ratio'].mean()

        improved_performance = {
            'features_tested': len(improved_results),
            'avg_ic': avg_ic,
            'avg_hit_ratio': avg_hit,
            'top_feature_ic': improved_results['holdout_ic_mean'].max(),
            'feature_details': improved_results.to_dict('records')
        }

        print(".4f")
        print(".1%")
        return improved_performance

    return None


def compare_before_after(baseline: Dict, improved: Dict) -> Dict:
    """
    ê°œì„  ì „í›„ ë¹„êµ ë¶„ì„

    Args:
        baseline: ê¸°ì¤€ ì„±ê³¼
        improved: ê°œì„  ì„±ê³¼

    Returns:
        ë¹„êµ ë¶„ì„ ê²°ê³¼
    """
    comparison = {
        'baseline_features': baseline['features_tested'],
        'improved_features': improved['features_tested'],
        'new_features_added': improved['features_tested'] - baseline['features_tested'],
        'ic_improvement': improved['avg_ic'] - baseline['avg_ic'],
        'ic_improvement_pct': ((improved['avg_ic'] - baseline['avg_ic']) / abs(baseline['avg_ic'])) * 100 if baseline['avg_ic'] != 0 else 0,
        'hit_ratio_improvement': improved['avg_hit_ratio'] - baseline['avg_hit_ratio'],
        'hit_ratio_improvement_pct': ((improved['avg_hit_ratio'] - baseline['avg_hit_ratio']) / baseline['avg_hit_ratio']) * 100 if baseline['avg_hit_ratio'] != 0 else 0,
        'top_ic_improvement': improved['top_feature_ic'] - baseline['top_feature_ic']
    }

    # ê°œì„  í‰ê°€
    if comparison['ic_improvement'] > 0.005:  # IC 0.005 ì´ìƒ ê°œì„ 
        comparison['overall_assessment'] = 'EXCELLENT'
    elif comparison['ic_improvement'] > 0.002:  # IC 0.002 ì´ìƒ ê°œì„ 
        comparison['overall_assessment'] = 'GOOD'
    elif comparison['ic_improvement'] > 0:
        comparison['overall_assessment'] = 'MODERATE'
    else:
        comparison['overall_assessment'] = 'NEEDS_IMPROVEMENT'

    return comparison


def generate_improvement_report(baseline: Dict, improved: Dict, comparison: Dict) -> str:
    """
    ê°œì„  íš¨ê³¼ ë³´ê³ ì„œ ìƒì„±

    Args:
        baseline: ê¸°ì¤€ ì„±ê³¼
        improved: ê°œì„  ì„±ê³¼
        comparison: ë¹„êµ ë¶„ì„

    Returns:
        ë³´ê³ ì„œ ë¬¸ìì—´
    """
    report = []
    report.append("# í”¼ì³ ê°œì„  íš¨ê³¼ ì¸¡ì • ë³´ê³ ì„œ")
    report.append("")
    report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")

    # ê°œìš”
    report.append("## ğŸ“‹ ë¶„ì„ ê°œìš”")
    report.append("")
    report.append("- **ê°œì„  ì „ í”¼ì³ ìˆ˜**: {}ê°œ".format(comparison['baseline_features']))
    report.append("- **ê°œì„  í›„ í”¼ì³ ìˆ˜**: {}ê°œ".format(comparison['improved_features']))
    report.append("- **ì¶”ê°€ëœ í”¼ì³ ìˆ˜**: {}ê°œ".format(comparison['new_features_added']))
    report.append("")

    # ì„±ê³¼ ë¹„êµ
    report.append("## ğŸ“Š ì„±ê³¼ ë¹„êµ")
    report.append("")
    report.append("| êµ¬ë¶„ | ê°œì„  ì „ | ê°œì„  í›„ | ê°œì„ ëŸ‰ | ê°œì„ ìœ¨ |")
    report.append("|------|--------|--------|--------|--------|")
    report.append("| í‰ê·  IC | {:.4f} | {:.4f} | {:.4f} | {:.1f}% |".format(
        baseline['avg_ic'], improved['avg_ic'],
        comparison['ic_improvement'], comparison['ic_improvement_pct']
    ))
    report.append("| í‰ê·  Hit Ratio | {:.1%} | {:.1%} | {:.1%} | {:.1f}% |".format(
        baseline['avg_hit_ratio'], improved['avg_hit_ratio'],
        comparison['hit_ratio_improvement'], comparison['hit_ratio_improvement_pct']
    ))
    report.append("")

    # í‰ê°€
    report.append("## ğŸ¯ ì¢…í•© í‰ê°€")
    report.append("")
    assessment = comparison['overall_assessment']
    if assessment == 'EXCELLENT':
        report.append("**â­ EXCELLENT**: IC ê°œì„ ì´ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤ (0.005+).")
    elif assessment == 'GOOD':
        report.append("**âœ… GOOD**: IC ê°œì„ ì´ ì–‘í˜¸í•©ë‹ˆë‹¤ (0.002-0.005).")
    elif assessment == 'MODERATE':
        report.append("**âš ï¸ MODERATE**: IC ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤ (0-0.002).")
    else:
        report.append("**âŒ NEEDS IMPROVEMENT**: ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    report.append("")

    # ìƒì„¸ ë¶„ì„
    report.append("## ğŸ” ìƒì„¸ ë¶„ì„")
    report.append("")

    # ìƒìœ„ ê°œì„  í”¼ì³ë“¤
    if baseline.get('feature_details') and improved.get('feature_details'):
        # ê°œì„ ëœ í”¼ì³ë“¤ì˜ IC ë¹„êµ
        baseline_df = pd.DataFrame(baseline['feature_details'])
        improved_df = pd.DataFrame(improved['feature_details'])

        # ê³µí†µ í”¼ì³ë“¤ ë¹„êµ
        common_features = set(baseline_df['feature_name']) & set(improved_df['feature_name'])
        if common_features:
            report.append("### ê¸°ì¡´ í”¼ì³ë“¤ì˜ ê°œì„  íš¨ê³¼")
            report.append("")
            report.append("| í”¼ì³ëª… | ê°œì„  ì „ IC | ê°œì„  í›„ IC | ê°œì„ ëŸ‰ |")
            report.append("|--------|-----------|-----------|--------|")

            for feature in list(common_features)[:10]:  # ìƒìœ„ 10ê°œë§Œ
                baseline_ic = baseline_df[baseline_df['feature_name'] == feature]['holdout_ic_mean'].iloc[0]
                improved_ic = improved_df[improved_df['feature_name'] == feature]['holdout_ic_mean'].iloc[0]
                improvement = improved_ic - baseline_ic

                report.append("| {} | {:.4f} | {:.4f} | {:.4f} |".format(
                    feature, baseline_ic, improved_ic, improvement
                ))

            report.append("")

        # ìƒˆë¡œìš´ í”¼ì³ë“¤ì˜ ì„±ê³¼
        new_features = set(improved_df['feature_name']) - set(baseline_df['feature_name'])
        if new_features:
            report.append("### ìƒˆë¡œìš´ í”¼ì³ë“¤ì˜ ì„±ê³¼")
            report.append("")
            report.append("| í”¼ì³ëª… | IC | Hit Ratio | í’ˆì§ˆ ì ìˆ˜ |")
            report.append("|--------|----|-----------|----------|")

            new_feature_results = improved_df[improved_df['feature_name'].isin(new_features)]
            top_new = new_feature_results.nlargest(10, 'quality_score')

            for _, row in top_new.iterrows():
                report.append("| {} | {:.4f} | {:.1%} | {:.1f} |".format(
                    row['feature_name'], row['holdout_ic_mean'],
                    row['holdout_hit_ratio'], row['quality_score']
                ))

            report.append("")

    # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
    report.append("## ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­")
    report.append("")

    if comparison['ic_improvement'] > 0:
        report.append("âœ… **í”¼ì³ ê°œì„ ì´ ê¸ì •ì ì¸ íš¨ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.**")
        report.append("- IC í‰ê· : {:.1f}% ê°œì„ ".format(comparison['ic_improvement_pct']))
        report.append("- Hit Ratio: {:.1f}% ê°œì„ ".format(comparison['hit_ratio_improvement_pct']))
        report.append("")
        report.append("**ì¶”ì²œ ì‚¬í•­**:")
        report.append("1. ê°œì„ ëœ í”¼ì³ì…‹ì„ ì •ì‹ìœ¼ë¡œ ì±„íƒ")
        report.append("2. ìƒìœ„ ì„±ê³¼ í”¼ì³ë“¤ì„ ìš°ì„  í™œìš©")
        report.append("3. ì¶”ê°€ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ê³ ë ¤")
    else:
        report.append("âš ï¸ **í”¼ì³ ê°œì„  íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.**")
        report.append("- ì¶”ê°€ì ì¸ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”")
        report.append("- ë‹¤ë¥¸ ê°œì„  ë°©í–¥ íƒìƒ‰ ê¶Œì¥")

    return "\n".join(report)


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ¯ í”¼ì³ ê°œì„  íš¨ê³¼ ì¸¡ì • ì‹œì‘")
    print("="*50)

    # ê°œì„  ì „ ì„±ê³¼ ì¸¡ì •
    print("\n[1/3] ê°œì„  ì „ ê¸°ì¤€ ì„±ê³¼ ì¸¡ì •...")
    baseline = measure_baseline_performance()

    if baseline is None:
        print("âŒ ê¸°ì¤€ ì„±ê³¼ ì¸¡ì • ì‹¤íŒ¨")
        return

    # ê°œì„  í›„ ì„±ê³¼ ì¸¡ì •
    print("\n[2/3] ê°œì„  í›„ ì„±ê³¼ ì¸¡ì •...")
    improved = measure_improved_performance()

    if improved is None:
        print("âŒ ê°œì„  ì„±ê³¼ ì¸¡ì • ì‹¤íŒ¨")
        return

    # ë¹„êµ ë¶„ì„
    print("\n[3/3] ê°œì„  ì „í›„ ë¹„êµ ë¶„ì„...")
    comparison = compare_before_after(baseline, improved)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*50)
    print(f"ê¸°ì¡´ í”¼ì³ ìˆ˜: {comparison['baseline_features']}")
    print(f"ê°œì„  í”¼ì³ ìˆ˜: {comparison['improved_features']}")
    print(f"ì¶”ê°€ í”¼ì³ ìˆ˜: {comparison['new_features_added']}")
    print(f"IC ê°œì„ : {comparison['ic_improvement']:.4f}")
    print(f"Hit Ratio ê°œì„ : {comparison['hit_ratio_improvement']:.1%}")
    print(f"ìµœê³  IC ê°œì„ : {comparison['top_ic_improvement']:.4f}")
    print(f"í‰ê°€: {comparison['overall_assessment']}")

    # ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
    report = generate_improvement_report(baseline, improved, comparison)

    # ì €ì¥
    cfg = load_config('configs/config.yaml')
    reports_dir = Path(cfg['paths']['base_dir']) / 'artifacts' / 'reports'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = reports_dir / f'feature_improvement_impact_{timestamp}.md'

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ’¾ ë³´ê³ ì„œ ì €ì¥: {report_file}")

    print("\nâœ… í”¼ì³ ê°œì„  íš¨ê³¼ ì¸¡ì • ì™„ë£Œ!")
    print("="*50)


if __name__ == "__main__":
    main()
