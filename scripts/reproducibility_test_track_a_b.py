# -*- coding: utf-8 -*-
"""
Track A/B ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

3ë²ˆ ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ Track A/B ë°±í…ŒìŠ¤íŠ¸ì˜ ì¬í˜„ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
L0~L6 ë°ì´í„°ëŠ” ê³ ì •ì‹œí‚¤ê³  L7 ë°±í…ŒìŠ¤íŠ¸ë§Œ ë°˜ë³µ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

from pathlib import Path
import shutil
import subprocess
import pandas as pd
import numpy as np
from datetime import datetime
import yaml
import sys

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

def clear_backtest_cache():
    """ë°±í…ŒìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ (L7 ê²°ê³¼ë§Œ)"""
    print("ğŸ§¹ ë°±í…ŒìŠ¤íŠ¸ ìºì‹œ ë°ì´í„° ì‚­ì œ ì¤‘...")

    # L7 ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ë§Œ ì‚­ì œ
    interim_dir = PROJECT_ROOT / 'data' / 'interim'
    reports_dir = PROJECT_ROOT / 'artifacts' / 'reports'

    # ì‚­ì œí•  íŒŒì¼ íŒ¨í„´ë“¤ (L7 ê²°ê³¼ë§Œ)
    patterns_to_remove = [
        'bt_metrics_*.parquet',
        'bt_metrics_*.csv',
        'bt_*_metrics*.parquet',
        'bt_*_metrics*.csv'
    ]

    # interim í´ë”ì—ì„œ ì‚­ì œ
    if interim_dir.exists():
        for pattern in patterns_to_remove:
            for file_path in interim_dir.glob(f'**/{pattern}'):
                if file_path.is_file():
                    file_path.unlink()
                    print(f"  ì‚­ì œ: {file_path.name}")

    # reports í´ë”ì—ì„œ ë¹„êµ íŒŒì¼ ì‚­ì œ
    if reports_dir.exists():
        comparison_file = reports_dir / 'backtest_4models_comparison.csv'
        if comparison_file.exists():
            comparison_file.unlink()
            print(f"  ì‚­ì œ: {comparison_file.name}")

    print("âœ… ë°±í…ŒìŠ¤íŠ¸ ìºì‹œ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

def run_backtest_iteration(iteration_num):
    """ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (L0~L6 ê³ ì •, L7ë§Œ ì¬ì‹¤í–‰)"""
    print(f"\n{'='*60}")
    print(f"ğŸ”„ ë°±í…ŒìŠ¤íŠ¸ ë°˜ë³µ ì‹¤í–‰ #{iteration_num}")
    print(f"{'='*60}")

    try:
        # 4ê°œ ëª¨ë¸ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (L0~L6 ë°ì´í„°ëŠ” ìœ ì§€)
        result = subprocess.run([
            sys.executable, 'scripts/run_backtest_4models.py'
        ], capture_output=True, text=True, cwd=PROJECT_ROOT)

        if result.returncode != 0:
            print(f"âŒ ë°±í…ŒìŠ¤íŠ¸ #{iteration_num} ì‹¤íŒ¨:")
            print(result.stderr)
            return None

        print(f"âœ… ë°±í…ŒìŠ¤íŠ¸ #{iteration_num} ì™„ë£Œ")

        # ê²°ê³¼ íŒŒì¼ ì½ê¸°
        result_file = PROJECT_ROOT / 'artifacts' / 'reports' / 'backtest_4models_comparison.csv'
        if result_file.exists():
            df = pd.read_csv(result_file)
            df['iteration'] = iteration_num
            return df
        else:
            print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {result_file}")
            return None

    except Exception as e:
        print(f"âŒ ë°±í…ŒìŠ¤íŠ¸ #{iteration_num} ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def analyze_backtest_reproducibility(results_df):
    """ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ì„± ë¶„ì„"""
    print("\nğŸ” Track A/B ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ì„± ë¶„ì„")
    print("="*80)

    if results_df is None or len(results_df) == 0:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì „ëµë³„ ë¶„ì„
    strategies = results_df['strategy'].unique()

    for strategy in strategies:
        strategy_data = results_df[results_df['strategy'] == strategy]

        print(f"\nğŸ¯ ì „ëµ: {strategy}")
        print("-" * 40)

        # ì£¼ìš” ë©”íŠ¸ë¦­ì˜ í†µê³„
        key_metrics = ['net_sharpe', 'net_cagr', 'net_mdd', 'net_calmar_ratio']

        reproducibility_issues = []

        for metric in key_metrics:
            if metric in strategy_data.columns:
                values = strategy_data[metric].values
                mean_val = np.mean(values)
                std_val = np.std(values)
                cv = std_val / abs(mean_val) if mean_val != 0 else 0

                print("12.4f"
                      "6.4f"
                      "8.1%")

                # ì¬í˜„ì„± í‰ê°€ (SharpeëŠ” 5%, ë‹¤ë¥¸ ì§€í‘œëŠ” 10% ë³€ë™ í—ˆìš©)
                threshold = 0.05 if 'sharpe' in metric else 0.10
                if cv > threshold:
                    reproducibility_issues.append(f"{metric}: {cv:.1%}")

        # ì¬í˜„ì„± í‰ê°€
        if not reproducibility_issues:
            reproducibility = "â­â­â­â­â­ EXCELLENT"
        elif len(reproducibility_issues) <= 1:
            reproducibility = "â­â­â­â­ GOOD"
        elif len(reproducibility_issues) <= 2:
            reproducibility = "â­â­â­ OK"
        else:
            reproducibility = "âš ï¸ POOR"

        print(f"ì¬í˜„ì„± í‰ê°€: {reproducibility} (ë¬¸ì œ ì§€í‘œ: {len(reproducibility_issues)}ê°œ)")
        if reproducibility_issues:
            for issue in reproducibility_issues:
                print(f"  - {issue}")

    # ì „ì²´ ìš”ì•½
    print("\nğŸ“Š ì „ì²´ ì¬í˜„ì„± ìš”ì•½")
    print("="*50)

    total_runs = len(results_df)
    unique_results = len(results_df.drop_duplicates(subset=['strategy', 'net_sharpe', 'net_cagr', 'net_mdd']))

    print(f"ì´ ì‹¤í–‰ íšŸìˆ˜: {total_runs}")
    print(f"ê³ ìœ  ê²°ê³¼ ìˆ˜: {unique_results}")
    print(f"ê²°ê³¼ ì¼ê´€ì„±: {unique_results}/{total_runs} ({unique_results/total_runs:.1%})")

    if unique_results == total_runs:
        print("ê²°ë¡ : âœ… ì™„ë²½í•œ ì¬í˜„ì„± (ëª¨ë“  ì‹¤í–‰ì—ì„œ ë™ì¼ ê²°ê³¼)")
    elif unique_results >= total_runs * 0.9:
        print("ê²°ë¡ : âš ï¸ ì–‘í˜¸í•œ ì¬í˜„ì„± (90% ì´ìƒ ì¼ê´€ì„±)")
    elif unique_results >= total_runs * 0.7:
        print("ê²°ë¡ : âŒ ì¬í˜„ì„± ë¬¸ì œ (70-90% ì¼ê´€ì„±)")
    else:
        print("ê²°ë¡ : âŒâŒ ì‹¬ê°í•œ ì¬í˜„ì„± ë¬¸ì œ (70% ë¯¸ë§Œ ì¼ê´€ì„±)")

def run_reproducibility_test(n_iterations=3):
    """Track A/B ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ì„± ê²€ì¦ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¬ Track A/B ë°±í…ŒìŠ¤íŠ¸ ì¬í˜„ì„± ê²€ì¦ ì‹œì‘")
    print(f"ë°˜ë³µ íšŸìˆ˜: {n_iterations}íšŒ")
    print("="*80)

    all_results = []

    for i in range(1, n_iterations + 1):
        # ë°±í…ŒìŠ¤íŠ¸ ìºì‹œ ë°ì´í„° ì‚­ì œ
        clear_backtest_cache()

        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result_df = run_backtest_iteration(i)

        if result_df is not None:
            all_results.append(result_df)
            print(f"âœ… #{i} ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            print(f"âŒ #{i} ì‹¤í–‰ ì‹¤íŒ¨")

        print(f"\nâ³ ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ 5ì´ˆ ëŒ€ê¸°...")
        import time
        time.sleep(5)

    # ê²°ê³¼ ë¶„ì„
    if len(all_results) == 0:
        print("âŒ ëª¨ë“  ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    # ê²°ê³¼ í†µí•©
    combined_results = pd.concat(all_results, ignore_index=True)

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = PROJECT_ROOT / 'artifacts' / 'reports' / f'backtest_reproducibility_test_{timestamp}.csv'
    combined_results.to_csv(output_file, index=False, encoding='utf-8-sig')

    print(f"\nâœ… ì¬í˜„ì„± ê²€ì¦ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ íŒŒì¼: {output_file}")
    print(f"ğŸ“ˆ ì´ ê²°ê³¼ ìˆ˜: {len(combined_results)}ê°œ")

    return combined_results

if __name__ == "__main__":
    # ì¬í˜„ì„± ê²€ì¦ ì‹¤í–‰
    results = run_reproducibility_test(n_iterations=3)

    # ê²°ê³¼ ë¶„ì„
    analyze_backtest_reproducibility(results)