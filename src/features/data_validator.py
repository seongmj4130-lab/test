# -*- coding: utf-8 -*-
"""
ë°ì´í„° ê²€ì¦ ìë™í™” ì‹œìŠ¤í…œ

í”¼ì³ ì¶”ê°€/ë³€ê²½ì— ëŒ€í•œ ìë™ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ë°ì´í„° ë¬´ê²°ì„±, í’ˆì§ˆ, ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


class DataValidator:
    """
    ë°ì´í„° ê²€ì¦ ìë™í™” í´ë˜ìŠ¤

    í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ì¼ê´€ì„±ì„ ìë™ìœ¼ë¡œ ê²€ì¦
    """

    def __init__(self):
        self.validation_results = {}

    def validate_data_integrity(
        self,
        df: pd.DataFrame,
        required_columns: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

        Args:
            df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
            required_columns: í•„ìˆ˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        integrity_result = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'data_types': df.dtypes.to_dict(),
            'missing_values': {},
            'duplicate_rows': df.duplicated().sum(),
            'required_columns_present': True,
            'critical_issues': []
        }

        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
        if required_columns:
            missing_required = [col for col in required_columns if col not in df.columns]
            if missing_required:
                integrity_result['required_columns_present'] = False
                integrity_result['critical_issues'].append(
                    f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_required}"
                )

        # ê²°ì¸¡ì¹˜ ë¶„ì„
        missing_stats = df.isnull().sum()
        integrity_result['missing_values'] = missing_stats.to_dict()

        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ë†’ì€ ì»¬ëŸ¼ë“¤
        missing_ratio = missing_stats / len(df)
        high_missing_cols = missing_ratio[missing_ratio > 0.5].index.tolist()
        if high_missing_cols:
            integrity_result['critical_issues'].append(
                f"ê²°ì¸¡ì¹˜ 50% ì´ìƒ ì»¬ëŸ¼: {high_missing_cols}"
            )

        # ì¤‘ë³µ í–‰ ê²€ì¦
        if integrity_result['duplicate_rows'] > 0:
            integrity_result['critical_issues'].append(
                f"ì¤‘ë³µ í–‰ ë°œê²¬: {integrity_result['duplicate_rows']}ê°œ"
            )

        return integrity_result

    def validate_feature_quality(
        self,
        df: pd.DataFrame,
        feature_columns: List[str]
    ) -> Dict[str, Any]:
        """
        í”¼ì³ í’ˆì§ˆ ê²€ì¦

        Args:
            df: í”¼ì³ ë°ì´í„°í”„ë ˆì„
            feature_columns: ê²€ì¦í•  í”¼ì³ ì»¬ëŸ¼ë“¤

        Returns:
            í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        quality_result = {
            'features_analyzed': len(feature_columns),
            'feature_stats': {},
            'quality_issues': [],
            'recommendations': []
        }

        for feature in feature_columns:
            if feature not in df.columns:
                quality_result['quality_issues'].append(f"í”¼ì³ ëˆ„ë½: {feature}")
                continue

            feature_data = df[feature]
            stats = {
                'count': len(feature_data),
                'missing': feature_data.isnull().sum(),
                'missing_ratio': feature_data.isnull().mean(),
                'unique_values': feature_data.nunique(),
                'dtype': str(feature_data.dtype)
            }

            # ìˆ˜ì¹˜í˜• í”¼ì³ ì¶”ê°€ í†µê³„
            if feature_data.dtype in [np.float64, np.float32, np.int64, np.int32]:
                stats.update({
                    'mean': feature_data.mean(),
                    'std': feature_data.std(),
                    'min': feature_data.min(),
                    'max': feature_data.max(),
                    'skewness': feature_data.skew(),
                    'kurtosis': feature_data.kurtosis(),
                    'zero_ratio': (feature_data == 0).mean(),
                    'outliers': self._detect_outliers(feature_data)
                })

                # í’ˆì§ˆ ì´ìŠˆ ê²€ì¶œ
                if stats['std'] == 0:
                    quality_result['quality_issues'].append(
                        f"{feature}: í‘œì¤€í¸ì°¨ê°€ 0 (ìƒìˆ˜ ê°’)"
                    )

                if stats['missing_ratio'] > 0.1:
                    quality_result['quality_issues'].append(
                        f"{feature}: ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë†’ìŒ ({stats['missing_ratio']:.1%})"
                    )

                if abs(stats['skewness']) > 3:
                    quality_result['recommendations'].append(
                        f"{feature}: ì™œë„ ë†’ìŒ ({stats['skewness']:.2f}), ë³€í™˜ ê³ ë ¤"
                    )

                if stats['outliers'] > len(feature_data) * 0.05:
                    quality_result['recommendations'].append(
                        f"{feature}: ì´ìƒì¹˜ ë§ìŒ ({stats['outliers']}ê°œ), ì²˜ë¦¬ ê³ ë ¤"
                    )

            quality_result['feature_stats'][feature] = stats

        return quality_result

    def _detect_outliers(self, series: pd.Series, method: str = 'iqr') -> int:
        """ì´ìƒì¹˜ ê²€ì¶œ"""
        if method == 'iqr':
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((series < lower_bound) | (series > upper_bound)).sum()
            return int(outliers)
        return 0

    def validate_feature_consistency(
        self,
        df: pd.DataFrame,
        feature_columns: List[str],
        groupby_column: str = 'ticker'
    ) -> Dict[str, Any]:
        """
        í”¼ì³ ì¼ê´€ì„± ê²€ì¦ (ì¢…ëª©ë³„/ì‹œê°„ë³„)

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            feature_columns: ê²€ì¦í•  í”¼ì³ë“¤
            groupby_column: ê·¸ë£¹í™” ê¸°ì¤€ ì»¬ëŸ¼

        Returns:
            ì¼ê´€ì„± ê²€ì¦ ê²°ê³¼
        """
        consistency_result = {
            'groupby_column': groupby_column,
            'feature_consistency': {},
            'temporal_consistency': {},
            'issues': []
        }

        if groupby_column not in df.columns:
            consistency_result['issues'].append(f"ê·¸ë£¹í™” ì»¬ëŸ¼ ì—†ìŒ: {groupby_column}")
            return consistency_result

        # ê·¸ë£¹ë³„ í”¼ì³ ì¼ê´€ì„±
        for feature in feature_columns:
            if feature not in df.columns:
                continue

            # ê·¸ë£¹ë³„ í†µê³„
            group_stats = df.groupby(groupby_column)[feature].agg([
                'count', 'mean', 'std', 'min', 'max'
            ])

            # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ì¢‹ìŒ)
            consistency_score = group_stats['std'].mean() / abs(group_stats['mean'].mean() + 1e-8)
            consistency_result['feature_consistency'][feature] = {
                'consistency_score': consistency_score,
                'groups_with_data': len(group_stats),
                'avg_group_size': group_stats['count'].mean()
            }

            # ì´ìŠˆ ê²€ì¶œ
            if consistency_score > 1.0:
                consistency_result['issues'].append(
                    f"{feature}: ê·¸ë£¹ê°„ ë³€ë™ì„± ë†’ìŒ (ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.2f})"
                )

        # ì‹œê°„ì  ì¼ê´€ì„± (ë‚ ì§œë³„)
        if 'date' in df.columns:
            for feature in feature_columns:
                if feature not in df.columns:
                    continue

                # ë‚ ì§œë³„ í†µê³„
                date_stats = df.groupby('date')[feature].agg(['count', 'mean', 'std'])
                date_consistency = date_stats['std'].mean() / abs(date_stats['mean'].mean() + 1e-8)

                consistency_result['temporal_consistency'][feature] = {
                    'temporal_consistency_score': date_consistency,
                    'dates_with_data': len(date_stats)
                }

                if date_consistency > 0.5:
                    consistency_result['issues'].append(
                        f"{feature}: ì‹œê°„ì  ë³€ë™ì„± ë†’ìŒ (ì ìˆ˜: {date_consistency:.2f})"
                    )

        return consistency_result

    def validate_feature_correlations(
        self,
        df: pd.DataFrame,
        feature_columns: List[str],
        target_column: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        í”¼ì³ ê°„ ìƒê´€ê´€ê³„ ë° íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ ê²€ì¦

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            feature_columns: í”¼ì³ ì»¬ëŸ¼ë“¤
            target_column: íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ (ì„ íƒ)

        Returns:
            ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼
        """
        correlation_result = {
            'feature_correlations': {},
            'target_correlations': {},
            'multicollinearity_issues': [],
            'high_correlation_pairs': []
        }

        # ìˆ˜ì¹˜í˜• í”¼ì³ë§Œ ì„ íƒ
        numeric_features = [col for col in feature_columns
                          if col in df.columns and
                          df[col].dtype in [np.float64, np.float32, np.int64, np.int32]]

        if len(numeric_features) < 2:
            correlation_result['multicollinearity_issues'].append("ë¶„ì„í•  ìˆ˜ì¹˜í˜• í”¼ì³ê°€ ë¶€ì¡±í•¨")
            return correlation_result

        # í”¼ì³ ê°„ ìƒê´€ê´€ê³„
        corr_matrix = df[numeric_features].corr()

        # ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìŒ ì°¾ê¸°
        high_corr_pairs = []
        for i in range(len(numeric_features)):
            for j in range(i+1, len(numeric_features)):
                corr = abs(corr_matrix.iloc[i, j])
                if corr > 0.8:  # ìƒê´€ê³„ìˆ˜ 0.8 ì´ìƒ
                    high_corr_pairs.append({
                        'feature1': numeric_features[i],
                        'feature2': numeric_features[j],
                        'correlation': corr
                    })

        correlation_result['high_correlation_pairs'] = high_corr_pairs

        if high_corr_pairs:
            correlation_result['multicollinearity_issues'].append(
                f"ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì³ ìŒ: {len(high_corr_pairs)}ê°œ ë°œê²¬"
            )

        # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„
        if target_column and target_column in df.columns:
            target_corr = df[numeric_features + [target_column]].corr()[target_column]
            correlation_result['target_correlations'] = target_corr.drop(target_column).to_dict()

            # íƒ€ê²Ÿê³¼ ë‚®ì€ ìƒê´€ê´€ê³„ í”¼ì³ë“¤
            low_corr_features = [feat for feat, corr in correlation_result['target_correlations'].items()
                               if abs(corr) < 0.01]
            if low_corr_features:
                correlation_result['multicollinearity_issues'].append(
                    f"íƒ€ê²Ÿê³¼ ë‚®ì€ ìƒê´€ê´€ê³„ í”¼ì³: {low_corr_features}"
                )

        return correlation_result

    def run_comprehensive_validation(
        self,
        df: pd.DataFrame,
        feature_columns: List[str],
        target_column: Optional[str] = None,
        required_columns: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ì¢…í•© ë°ì´í„° ê²€ì¦ ì‹¤í–‰

        Args:
            df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
            feature_columns: í”¼ì³ ì»¬ëŸ¼ë“¤
            target_column: íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼
            required_columns: í•„ìˆ˜ ì»¬ëŸ¼ë“¤

        Returns:
            ì¢…í•© ê²€ì¦ ê²°ê³¼
        """
        print("ğŸ” ë°ì´í„° ê²€ì¦ ì‹œì‘...")

        comprehensive_result = {
            'timestamp': pd.Timestamp.now(),
            'summary': {},
            'details': {}
        }

        # 1. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        print("  ğŸ“‹ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
        integrity_result = self.validate_data_integrity(df, required_columns)
        comprehensive_result['details']['integrity'] = integrity_result

        # 2. í”¼ì³ í’ˆì§ˆ ê²€ì¦
        print("  ğŸ“Š í”¼ì³ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        quality_result = self.validate_feature_quality(df, feature_columns)
        comprehensive_result['details']['quality'] = quality_result

        # 3. í”¼ì³ ì¼ê´€ì„± ê²€ì¦
        print("  ğŸ”„ í”¼ì³ ì¼ê´€ì„± ê²€ì¦ ì¤‘...")
        consistency_result = self.validate_feature_consistency(df, feature_columns)
        comprehensive_result['details']['consistency'] = consistency_result

        # 4. í”¼ì³ ìƒê´€ê´€ê³„ ê²€ì¦
        print("  ğŸ“ˆ í”¼ì³ ìƒê´€ê´€ê³„ ê²€ì¦ ì¤‘...")
        correlation_result = self.validate_feature_correlations(df, feature_columns, target_column)
        comprehensive_result['details']['correlation'] = correlation_result

        # ê²€ì¦ ìš”ì•½ ìƒì„±
        comprehensive_result['summary'] = self._generate_validation_summary(
            integrity_result, quality_result, consistency_result, correlation_result
        )

        print("âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        return comprehensive_result

    def _generate_validation_summary(
        self,
        integrity: Dict,
        quality: Dict,
        consistency: Dict,
        correlation: Dict
    ) -> Dict[str, Any]:
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±"""

        # ì‹¬ê°ë„ë³„ ì´ìŠˆ ë¶„ë¥˜
        critical_issues = []
        warnings = []
        recommendations = []

        # ë¬´ê²°ì„± ì´ìŠˆ
        critical_issues.extend(integrity.get('critical_issues', []))

        # í’ˆì§ˆ ì´ìŠˆ
        critical_issues.extend(quality.get('quality_issues', []))
        recommendations.extend(quality.get('recommendations', []))

        # ì¼ê´€ì„± ì´ìŠˆ
        warnings.extend(consistency.get('issues', []))

        # ìƒê´€ê´€ê³„ ì´ìŠˆ
        warnings.extend(correlation.get('multicollinearity_issues', []))

        summary = {
            'overall_status': 'PASS' if not critical_issues else 'FAIL',
            'total_features': quality.get('features_analyzed', 0),
            'critical_issues_count': len(critical_issues),
            'warning_count': len(warnings),
            'recommendation_count': len(recommendations),
            'data_quality_score': self._calculate_data_quality_score(
                integrity, quality, consistency, correlation
            ),
            'critical_issues': critical_issues[:5],  # ìƒìœ„ 5ê°œë§Œ
            'warnings': warnings[:5],
            'recommendations': recommendations[:5]
        }

        return summary

    def _calculate_data_quality_score(
        self,
        integrity: Dict,
        quality: Dict,
        consistency: Dict,
        correlation: Dict
    ) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""

        score = 100.0

        # ë¬´ê²°ì„± ì ìˆ˜ ì°¨ê°
        missing_ratio = sum(integrity.get('missing_values', {}).values()) / (integrity.get('total_rows', 1) * integrity.get('total_columns', 1))
        score -= missing_ratio * 20  # ê²°ì¸¡ì¹˜ 5%ë‹¹ 1ì  ì°¨ê°

        if integrity.get('duplicate_rows', 0) > 0:
            score -= 5

        # í’ˆì§ˆ ì ìˆ˜ ì°¨ê°
        quality_issues = len(quality.get('quality_issues', []))
        score -= quality_issues * 10  # í’ˆì§ˆ ì´ìŠˆë‹¹ 10ì  ì°¨ê°

        # ì¼ê´€ì„± ì ìˆ˜ ì°¨ê°
        consistency_issues = len(consistency.get('issues', []))
        score -= consistency_issues * 5  # ì¼ê´€ì„± ì´ìŠˆë‹¹ 5ì  ì°¨ê°

        # ìƒê´€ê´€ê³„ ì ìˆ˜ ì°¨ê°
        correlation_issues = len(correlation.get('multicollinearity_issues', []))
        score -= correlation_issues * 5  # ìƒê´€ê´€ê³„ ì´ìŠˆë‹¹ 5ì  ì°¨ê°

        return max(0, min(100, score))

    def generate_validation_report(self, validation_result: Dict) -> str:
        """ê²€ì¦ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""

        report = []
        report.append("# ë°ì´í„° ê²€ì¦ ìë™í™” ë³´ê³ ì„œ")
        report.append("")
        report.append(f"ìƒì„± ì‹œê°„: {validation_result['timestamp']}")
        report.append("")

        # ìš”ì•½ ì„¹ì…˜
        summary = validation_result['summary']
        report.append("## ê²€ì¦ ìš”ì•½")
        report.append("")
        report.append(f"- **ì „ì²´ ìƒíƒœ**: {summary['overall_status']}")
        report.append(f"- **ë¶„ì„ í”¼ì³ ìˆ˜**: {summary['total_features']}")
        report.append(f"- **ë°ì´í„° í’ˆì§ˆ ì ìˆ˜**: {summary['data_quality_score']:.1f}/100")
        report.append(f"- **ì‹¬ê°í•œ ì´ìŠˆ**: {summary['critical_issues_count']}ê°œ")
        report.append(f"- **ê²½ê³ **: {summary['warning_count']}ê°œ")
        report.append(f"- **ê¶Œì¥ì‚¬í•­**: {summary['recommendation_count']}ê°œ")
        report.append("")

        # ìƒì„¸ ì„¹ì…˜
        if summary['critical_issues']:
            report.append("## ğŸš¨ ì‹¬ê°í•œ ì´ìŠˆ")
            report.append("")
            for issue in summary['critical_issues']:
                report.append(f"- {issue}")
            report.append("")

        if summary['warnings']:
            report.append("## âš ï¸ ê²½ê³ ")
            report.append("")
            for warning in summary['warnings']:
                report.append(f"- {warning}")
            report.append("")

        if summary['recommendations']:
            report.append("## ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            report.append("")
            for rec in summary['recommendations']:
                report.append(f"- {rec}")
            report.append("")

        return "\n".join(report)


def test_data_validator():
    """ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    from src.utils.config import load_config
    from src.utils.io import load_artifact
    from pathlib import Path

    # ì„¤ì • ë¡œë“œ
    cfg = load_config('configs/config.yaml')
    interim_dir = Path(cfg['paths']['base_dir']) / 'data' / 'interim'

    # ë°ì´í„° ë¡œë“œ
    panel_df = load_artifact(interim_dir / 'panel_merged_daily')

    if panel_df is None:
        print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return

    # í…ŒìŠ¤íŠ¸í•  í”¼ì³ë“¤
    test_features = [
        'close_to_52w_high', 'close_to_52w_low', 'intraday_price_position',
        'momentum_3m_ewm', 'momentum_6m_ewm', 'momentum_3m_vol_adj',
        'volatility_asymmetry', 'tail_risk_5pct'
    ]

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì³ë“¤ë§Œ
    available_features = [f for f in test_features if f in panel_df.columns]

    if not available_features:
        print("í…ŒìŠ¤íŠ¸í•  í”¼ì³ê°€ ì—†ìŒ")
        return

    # ê²€ì¦ ì‹¤í–‰
    validator = DataValidator()
    validation_result = validator.run_comprehensive_validation(
        df=panel_df,
        feature_columns=available_features,
        required_columns=['date', 'ticker', 'close'],
        target_column=None
    )

    # ë³´ê³ ì„œ ìƒì„±
    report = validator.generate_validation_report(validation_result)
    print("=== ë°ì´í„° ê²€ì¦ ë³´ê³ ì„œ ===")
    print(report)

    return validation_result


if __name__ == "__main__":
    test_data_validator()