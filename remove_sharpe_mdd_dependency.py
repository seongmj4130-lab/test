#!/usr/bin/env python3
"""
ê³¼ë„í•œ Sharpe/MDD ì˜ì¡´ì„± ì œê±° - ìˆ˜ìµë¥  ì¤‘ì‹¬ í‰ê°€ ì‹œìŠ¤í…œ
"""

from pathlib import Path

import numpy as np
import pandas as pd
import yaml


class ReturnFocusedEvaluator:
    """ìˆ˜ìµë¥  ì¤‘ì‹¬ í‰ê°€ ì‹œìŠ¤í…œ (Sharpe/MDD ì˜ì¡´ì„± ì œê±°)"""

    def __init__(self):
        self.old_weights = {
            'cagr': 0.20,        # ë‚®ì€ ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜
            'total_return': 0.15, # ë‚®ì€ ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜
            'sharpe': 0.35,      # ë†’ì€ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ë¬¸ì œ!)
            'mdd': 0.20,         # ë†’ì€ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ë¬¸ì œ!)
            'calmar': 0.10       # ë³´ì¡° ì§€í‘œ
        }

        self.new_weights = {
            'cagr': 0.45,        # ë†’ì€ ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜ (ì¦ê°€)
            'total_return': 0.30, # ë†’ì€ ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜ (ì¦ê°€)
            'sharpe': 0.10,      # ë‚®ì€ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ê°ì†Œ)
            'mdd': 0.10,         # ë‚®ì€ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ê°ì†Œ)
            'calmar': 0.05       # ë³´ì¡° ì§€í‘œ (ê°ì†Œ)
        }

    def compare_evaluation_systems(self):
        """ê¸°ì¡´ vs ìƒˆë¡œìš´ í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ"""

        print("ğŸ”„ ê³¼ë„í•œ Sharpe/MDD ì˜ì¡´ì„± ì œê±°")
        print("="*60)

        print("ğŸ“Š í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ")
        print("-" * 60)
        print("êµ¬ë¶„".ljust(15), "ê¸°ì¡´ ì‹œìŠ¤í…œ".ljust(15), "ìƒˆë¡œìš´ ì‹œìŠ¤í…œ".ljust(15), "ë³€í™”")
        print("-" * 60)

        for metric in self.old_weights.keys():
            old_w = self.old_weights[metric]
            new_w = self.new_weights[metric]
            change = new_w - old_w
            change_str = f"{change:+.0%}"

            print(f"{metric.ljust(15)} {old_w:>13.0%} {new_w:>13.0%} {change_str:>10}")

        print("\nğŸ’¡ ì‹œìŠ¤í…œ ë³€í™” ì„¤ëª…:")
        print("  â€¢ CAGR ê°€ì¤‘ì¹˜: 20% â†’ 45% (+25%p) - ì ˆëŒ€ ìˆ˜ìµë¥  ìµœìš°ì„ ")
        print("  â€¢ Total Return ê°€ì¤‘ì¹˜: 15% â†’ 30% (+15%p) - ëˆ„ì  ìˆ˜ìµë¥  ì¤‘ìš”ì„± ì¦ê°€")
        print("  â€¢ Sharpe ê°€ì¤‘ì¹˜: 35% â†’ 10% (-25%p) - ë¦¬ìŠ¤í¬ ì¡°ì • ì˜ì¡´ì„± ì œê±°")
        print("  â€¢ MDD ê°€ì¤‘ì¹˜: 20% â†’ 10% (-10%p) - í•˜ë½ ìœ„í—˜ ì˜ì¡´ì„± ì œê±°")
        print("  â€¢ Calmar ê°€ì¤‘ì¹˜: 10% â†’ 5% (-5%p) - ë³µí•© ì§€í‘œ ì˜í–¥ ê°ì†Œ")

    def demonstrate_evaluation_difference(self):
        """ê¸°ì¡´ vs ìƒˆë¡œìš´ í‰ê°€ ë°©ì‹ ì°¨ì´ ì‹œì—°"""

        print("\nğŸ¯ í‰ê°€ ë°©ì‹ ì°¨ì´ ì‹œì—°")
        print("-" * 60)

        # ìƒ˜í”Œ ì „ëµ ë°ì´í„° (ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜)
        sample_strategies = {
            'High_Return_Low_Risk': {
                'cagr': 8.0, 'total_return': 15.0, 'sharpe': 1.2, 'mdd': -8.0, 'calmar': 1.0
            },
            'Moderate_Return_High_Risk': {
                'cagr': 5.0, 'total_return': 9.0, 'sharpe': 0.8, 'mdd': -15.0, 'calmar': 0.33
            },
            'Low_Return_Low_Risk': {
                'cagr': 2.0, 'total_return': 4.0, 'sharpe': 1.5, 'mdd': -5.0, 'calmar': 0.4
            }
        }

        print("ì „ëµë³„ í‰ê°€ ì ìˆ˜ ë¹„êµ:")
        print("ì „ëµ".ljust(25), "ê¸°ì¡´ì ìˆ˜".ljust(10), "ìƒˆì ìˆ˜".ljust(10), "ìˆœìœ„ë³€í™”")
        print("-" * 60)

        results = []
        for name, metrics in sample_strategies.items():
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ì ìˆ˜ ê³„ì‚°
            old_score = sum(metrics[k] * self.old_weights[k] for k in self.old_weights.keys())
            old_score = (old_score / 10) * 100  # ì •ê·œí™” (0-100ì )

            # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ì ìˆ˜ ê³„ì‚°
            new_score = sum(metrics[k] * self.new_weights[k] for k in self.new_weights.keys())
            new_score = (new_score / 10) * 100  # ì •ê·œí™” (0-100ì )

            results.append((name, old_score, new_score))

        # ê¸°ì¡´ ì‹œìŠ¤í…œ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x[1], reverse=True)

        for name, old_score, new_score in results:
            rank_change = ""
            print(f"{name.ljust(25)} {old_score:>8.1f} {new_score:>8.1f} {rank_change}")

        print("\nğŸ’¡ í‰ê°€ ê²°ê³¼ í•´ì„:")
        print("  â€¢ ê¸°ì¡´ ì‹œìŠ¤í…œ: ë¦¬ìŠ¤í¬ ì§€í‘œ(Sharpe/MDD)ê°€ 55% ì°¨ì§€")
        print("  â€¢ ìƒˆë¡œìš´ ì‹œìŠ¤í…œ: ìˆ˜ìµë¥  ì§€í‘œ(CAGR/Total Return)ê°€ 75% ì°¨ì§€")
        print("  â€¢ ê²°ê³¼: ì ˆëŒ€ ìˆ˜ìµë¥ ì´ ë‚®ì•„ë„ ë¦¬ìŠ¤í¬ê°€ ì¢‹ìœ¼ë©´ ê³ í‰ê°€ë˜ë˜ ë¬¸ì œ í•´ê²°")

    def update_evaluation_config(self):
        """ì„¤ì • íŒŒì¼ì— ìƒˆë¡œìš´ í‰ê°€ ê°€ì¤‘ì¹˜ ì ìš©"""

        config_path = 'configs/config.yaml'

        try:
            # ê¸°ì¡´ ì„¤ì • ë¡œë“œ
            if Path(config_path).exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            else:
                config = {}

            # ìƒˆë¡œìš´ í‰ê°€ ê°€ì¤‘ì¹˜ ì„¤ì •
            if 'evaluation' not in config:
                config['evaluation'] = {}

            config['evaluation']['weights'] = {
                'description': 'ìˆ˜ìµë¥  ì¤‘ì‹¬ í‰ê°€ ì‹œìŠ¤í…œ (Sharpe/MDD ì˜ì¡´ì„± ì œê±°)',
                'cagr': 0.45,
                'total_return': 0.30,
                'sharpe': 0.10,
                'mdd': 0.10,
                'calmar': 0.05,
                'last_updated': '2025-01-14',
                'change_reason': 'ê³¼ë„í•œ Sharpe/MDD ì˜ì¡´ì„± ì œê±°, ì ˆëŒ€ ìˆ˜ìµë¥  ì¤‘ì‹¬ í‰ê°€'
            }

            # ì„¤ì • ì €ì¥
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)

            print("âœ… config.yamlì— ìƒˆë¡œìš´ í‰ê°€ ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£Œ")
            print(f"ğŸ“ íŒŒì¼: {config_path}")

        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def validate_real_strategy_evaluation(self):
        """ì‹¤ì œ ì „ëµì— ìƒˆë¡œìš´ í‰ê°€ ë°©ì‹ ì ìš©"""

        print("\nğŸ¯ ì‹¤ì œ ì „ëµ ì¬í‰ê°€ (ìƒˆë¡œìš´ ì‹œìŠ¤í…œ)")
        print("-" * 60)

        # ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ
        results_path = "results/final_18_cases_backtest_report_20260114_030411.csv"
        if not Path(results_path).exists():
            print("âŒ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.read_csv(results_path)

        # ë²¤ì¹˜ë§ˆí¬
        kospi_return = 4.5
        quant_avg_return = 6.5

        print("ì „ëµë³„ ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼:")
        print("ì „ëµ".ljust(15), "CAGR".ljust(8), "ì´ìˆ˜ìµ".ljust(8), "Sharpe".ljust(8), "MDD".ljust(8), "ìƒˆì ìˆ˜".ljust(8), "ë“±ê¸‰")
        print("-" * 80)

        for strategy in ['bt20_short', 'bt20_ens', 'bt120_long']:
            strategy_data = df[df['strategy'] == strategy]

            if strategy_data.empty:
                continue

            # ìµœê³  CAGR ì¼€ì´ìŠ¤ ì„ íƒ
            best_idx = strategy_data['cagr(%)'].idxmax()
            best_case = strategy_data.loc[best_idx]

            # ìƒˆë¡œìš´ í‰ê°€ ë°©ì‹ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
            cagr_score = (best_case['cagr(%)'] / 10) * 100  # CAGR ê¸°ë°˜ ì •ê·œí™”
            total_return_score = (best_case['total_return(%)'] / 20) * 100  # ì´ìˆ˜ìµ ê¸°ë°˜ ì •ê·œí™”

            new_score = (
                best_case['cagr(%)'] * self.new_weights['cagr'] +
                best_case['total_return(%)'] * self.new_weights['total_return'] +
                best_case['sharpe'] * self.new_weights['sharpe'] +
                abs(best_case['mdd(%)']) * self.new_weights['mdd'] +  # MDDëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                best_case['calmar'] * self.new_weights['calmar']
            )

            # ë“±ê¸‰ ê²°ì • (ìˆ˜ìµë¥  ì¤‘ì‹¬)
            if best_case['cagr(%)'] >= quant_avg_return:
                grade = "A"
            elif best_case['cagr(%)'] >= kospi_return:
                grade = "B"
            elif best_case['cagr(%)'] >= 0:
                grade = "C"
            else:
                grade = "D"

            print(f"{strategy.ljust(15)} {best_case['cagr(%)']:>6.2f} {best_case['total_return(%)']:>6.2f} {best_case['sharpe']:>6.2f} {best_case['mdd(%)']:>6.2f} {new_score:>6.1f} {grade}")

        print("\nğŸ’¡ ìƒˆë¡œìš´ í‰ê°€ ë°©ì‹ì˜ íŠ¹ì§•:")
        print("  â€¢ CAGRì™€ ì´ìˆ˜ìµì´ 75%ì˜ ê°€ì¤‘ì¹˜ ì°¨ì§€")
        print("  â€¢ Sharpe/MDD ì˜í–¥ë ¥ì´ 20%ë¡œ ì¶•ì†Œ")
        print("  â€¢ ì ˆëŒ€ ìˆ˜ìµë¥ ì´ ë‚®ìœ¼ë©´ ë“±ê¸‰ ìë™ í•˜ë½")
        print("  â€¢ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì¢‹ì€ë° ìˆ˜ìµë¥  ë‚®ì€ ì „ëµ ê±¸ëŸ¬ëƒ„")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    evaluator = ReturnFocusedEvaluator()

    # 1. í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ
    evaluator.compare_evaluation_systems()

    # 2. í‰ê°€ ë°©ì‹ ì°¨ì´ ì‹œì—°
    evaluator.demonstrate_evaluation_difference()

    # 3. ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
    evaluator.update_evaluation_config()

    # 4. ì‹¤ì œ ì „ëµ ì¬í‰ê°€
    evaluator.validate_real_strategy_evaluation()

    print("\nâœ… ê³¼ë„í•œ Sharpe/MDD ì˜ì¡´ì„± ì œê±° ì™„ë£Œ!")
    print("ğŸ¯ í‰ê°€ ì‹œìŠ¤í…œ: ìˆ˜ìµë¥  ì¤‘ì‹¬ìœ¼ë¡œ ì „í™˜ë¨")

if __name__ == "__main__":
    main()
